/*
Boost Software License - Version 1.0 - August 17th, 2003
Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license ( the "Software" ) to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:
The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
*/
module derelict.cuda.runtimeapi;

/*
 * CUDA Runtime API
 * Translation of cuda_runtime_api.h and headers included by it
 */

// Current API version supported by DerelictCUDA is 6.5
enum CUDART_VERSION = 10000;

import derelict.util.loader;

private
{
  import derelict.util.system;

  static if(Derelict_OS_Windows)
    enum libNames = "cudart32_100.dll,cudart64_100.dll";
  else static if (Derelict_OS_Mac)
    enum libNames = "libcudart.dylib,/usr/local/lib/libcudart.dylib";
  else static if (Derelict_OS_Linux)
  {
    version(X86)
      enum libNames = "libcudart.so,libcudart.so.10.0,/opt/cuda/lib/libcudart.so";
    else version(X86_64)
      enum libNames = "libcudart.so,libcudart.so.10.0,/opt/cuda/lib64/libcudart.so,/usr/lib/x86_64-linux-gnu/libcudart.so.10.0";
    else
      static assert(0, "Need to implement CUDA libNames for this arch.");
  }
  else
    static assert(0, "Need to implement CUDA libNames for this operating system.");
}


// library_types.h


alias cudaDataType = int;
enum : cudaDataType {
	CUDA_R_16F= 2,  /* real as a half */
	CUDA_C_16F= 6,  /* complex as a pair of half numbers */
	CUDA_R_32F= 0,  /* real as a float */
	CUDA_C_32F= 4,  /* complex as a pair of float numbers */
	CUDA_R_64F= 1,  /* real as a double */
	CUDA_C_64F= 5,  /* complex as a pair of double numbers */
	CUDA_R_8I = 3,  /* real as a signed char */
	CUDA_C_8I = 7,  /* complex as a pair of signed char numbers */
	CUDA_R_8U = 8,  /* real as a dchar */
	CUDA_C_8U = 9,  /* complex as a pair of dchar numbers */
	CUDA_R_32I= 10, /* real as a signed int */
	CUDA_C_32I= 11, /* complex as a pair of signed int numbers */
	CUDA_R_32U= 12, /* real as a uint */
	CUDA_C_32U= 13  /* complex as a pair of uint numbers */
}


alias libraryPropertyType = int;
enum : libraryPropertyType {
	MAJOR_VERSION,
	MINOR_VERSION,
	PATCH_LEVEL
}




//struct cuComplex;
//struct cuDoubleComplex;





// vector_types.h
// only dim3 translated
// TODO(naetherm): Are the other required as well?
struct dim3 {
  uint x = 1,
       y = 1,
       z = 1;
}

struct float2 {
  float x, y;
}

struct double2 {
  double x, y;
}

// device_types.h

alias cudaRoundMode = int;
enum : cudaRoundMode
{
  cudaRoundNearest,
  cudaRoundZero,
  cudaRoundPosInf,
  cudaRoundMinInf
}

// driver_types.h

enum cudaHostAllocDefault           = 0x00;
enum cudaHostAllocPortable          = 0x01;
enum cudaHostAllocMapped            = 0x02;
enum cudaHostAllocWriteCombined     = 0x04;
enum cudaHostRegisterDefault        = 0x00;
enum cudaHostRegisterPortable       = 0x01;
enum cudaHostRegisterMapped         = 0x02;
enum cudaHosatRegisterIoMemory      = 0x04;

enum cudaPeerAccessDefault          = 0x00;

enum cudaStreamDefault              = 0x00;
enum cudaStreamNonBlocking          = 0x01;

//#define cudaStreamLegacy                    ((cudaStream_t)0x1)
//#define cudaStreamPerThread                 ((cudaStream_t)0x2)

enum cudaEventDefault               = 0x00;
enum cudaEventBlockingSync          = 0x01;
enum cudaEventDisableTiming         = 0x02;
enum cudaEventInterprocess          = 0x04;

enum cudaDeviceScheduleAuto         = 0x00;
enum cudaDeviceScheduleSpin         = 0x01;
enum cudaDeviceScheduleYield        = 0x02;
enum cudaDeviceScheduleBlockingSync = 0x04;
enum cudaDeviceBlockingSync         = 0x04;

enum cudaDeviceScheduleMask         = 0x07;
enum cudaDeviceMapHost              = 0x08;
enum cudaDeviceLmemResizeToMax      = 0x10;
enum cudaDeviceMask                 = 0x1f;

enum cudaArrayDefault               = 0x00;
enum cudaArrayLayered               = 0x01;
enum cudaArraySurfaceLoadStore      = 0x02;
enum cudaArrayCubemap               = 0x04;
enum cudaArrayTextureGather         = 0x08;
enum cudaArrayColorAttachment       = 0x20;

enum cudaIpcMemLazyEnablePeerAccess = 0x01;

enum cudaMemAttachGlobal            = 0x01;
enum cudaMemAttachHost              = 0x02;
enum cudaMemAttachSingle            = 0x04;

enum cudaCpuDeviceId               = (cast(uint)-1);
enum cudaInvalidDeviceId           = (cast(uint)-2);

enum cudaCooperativeLaunchMultiDeviceNoPreSync  = 0x01;
enum cudaCooperativeLaunchMultiDeviceNoPostSync = 0x02;


alias cudaError = int;
enum : cudaError
{
  /**
   * The API call returned with no errors. In the case of query calls, this
   * also means that the operation being queried is complete (see
   * ::cudaEventQuery() and ::cudaStreamQuery()).
   */
  cudaSuccess                           =      0,

  /**
   * The device function being invoked (usually via ::cudaLaunchKernel()) was not
   * previously configured via the ::cudaConfigureCall() function.
   */
  cudaErrorMissingConfiguration         =      1,

  /**
   * The API call failed because it was unable to allocate enough memory to
   * perform the requested operation.
   */
  cudaErrorMemoryAllocation             =      2,

  /**
   * The API call failed because the CUDA driver and runtime could not be
   * initialized.
   */
  cudaErrorInitializationError          =      3,

  /**
   * An exception occurred on the device while executing a kernel. Common
   * causes include dereferencing an invalid device pointer and accessing
   * out of bounds shared memory. All existing device memory allocations
   * are invalid. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorLaunchFailure                =      4,

  /**
   * This indicated that a previous kernel launch failed. This was previously
   * used for device emulation of kernel launches.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Device emulation mode was
   * removed with the CUDA 3.1 release.
   */
  cudaErrorPriorLaunchFailure           =      5,

  /**
   * This indicates that the device kernel took too long to execute. This can
   * only occur if timeouts are enabled - see the device property
   * \ref ::cudaDeviceProp::kernelExecTimeoutEnabled "kernelExecTimeoutEnabled"
   * for more information.
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorLaunchTimeout                =      6,

  /**
   * This indicates that a launch did not occur because it did not have
   * appropriate resources. Although this error is similar to
   * ::cudaErrorInvalidConfiguration, this error usually indicates that the
   * user has attempted to pass too many arguments to the device kernel, or the
   * kernel launch specifies too many threads for the kernel's register count.
   */
  cudaErrorLaunchOutOfResources         =      7,

  /**
   * The requested device function does not exist or is not compiled for the
   * proper device architecture.
   */
  cudaErrorInvalidDeviceFunction        =      8,

  /**
   * This indicates that a kernel launch is requesting resources that can
   * never be satisfied by the current device. Requesting more shared memory
   * per block than the device supports will trigger this error, as will
   * requesting too many threads or blocks. See ::cudaDeviceProp for more
   * device limitations.
   */
  cudaErrorInvalidConfiguration         =      9,

  /**
   * This indicates that the device ordinal supplied by the user does not
   * correspond to a valid CUDA device.
   */
  cudaErrorInvalidDevice                =     10,

  /**
   * This indicates that one or more of the parameters passed to the API call
   * is not within an acceptable range of values.
   */
  cudaErrorInvalidValue                 =     11,

  /**
   * This indicates that one or more of the pitch-related parameters passed
   * to the API call is not within the acceptable range for pitch.
   */
  cudaErrorInvalidPitchValue            =     12,

  /**
   * This indicates that the symbol name/identifier passed to the API call
   * is not a valid name or identifier.
   */
  cudaErrorInvalidSymbol                =     13,

  /**
   * This indicates that the buffer object could not be mapped.
   */
  cudaErrorMapBufferObjectFailed        =     14,

  /**
   * This indicates that the buffer object could not be unmapped.
   */
  cudaErrorUnmapBufferObjectFailed      =     15,

  /**
   * This indicates that at least one host pointer passed to the API call is
   * not a valid host pointer.
   */
  cudaErrorInvalidHostPointer           =     16,

  /**
   * This indicates that at least one device pointer passed to the API call is
   * not a valid device pointer.
   */
  cudaErrorInvalidDevicePointer         =     17,

  /**
   * This indicates that the texture passed to the API call is not a valid
   * texture.
   */
  cudaErrorInvalidTexture               =     18,

  /**
   * This indicates that the texture binding is not valid. This occurs if you
   * call ::cudaGetTextureAlignmentOffset() with an unbound texture.
   */
  cudaErrorInvalidTextureBinding        =     19,

  /**
   * This indicates that the channel descriptor passed to the API call is not
   * valid. This occurs if the format is not one of the formats specified by
   * ::cudaChannelFormatKind, or if one of the dimensions is invalid.
   */
  cudaErrorInvalidChannelDescriptor     =     20,

  /**
   * This indicates that the direction of the memcpy passed to the API call is
   * not one of the types specified by ::cudaMemcpyKind.
   */
  cudaErrorInvalidMemcpyDirection       =     21,

  /**
   * This indicated that the user has taken the address of a constant variable,
   * which was forbidden up until the CUDA 3.1 release.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Variables in constant
   * memory may now have their address taken by the runtime via
   * ::cudaGetSymbolAddress().
   */
  cudaErrorAddressOfConstant            =     22,

  /**
   * This indicated that a texture fetch was not able to be performed.
   * This was previously used for device emulation of texture operations.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Device emulation mode was
   * removed with the CUDA 3.1 release.
   */
  cudaErrorTextureFetchFailed           =     23,

  /**
   * This indicated that a texture was not bound for access.
   * This was previously used for device emulation of texture operations.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Device emulation mode was
   * removed with the CUDA 3.1 release.
   */
  cudaErrorTextureNotBound              =     24,

  /**
   * This indicated that a synchronization operation had failed.
   * This was previously used for some device emulation functions.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Device emulation mode was
   * removed with the CUDA 3.1 release.
   */
  cudaErrorSynchronizationError         =     25,

  /**
   * This indicates that a non-float texture was being accessed with linear
   * filtering. This is not supported by CUDA.
   */
  cudaErrorInvalidFilterSetting         =     26,

  /**
   * This indicates that an attempt was made to read a non-float texture as a
   * normalized float. This is not supported by CUDA.
   */
  cudaErrorInvalidNormSetting           =     27,

  /**
   * Mixing of device and device emulation code was not allowed.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Device emulation mode was
   * removed with the CUDA 3.1 release.
   */
  cudaErrorMixedDeviceExecution         =     28,

  /**
   * This indicates that a CUDA Runtime API call cannot be executed because
   * it is being called during process shut down, at a point in time after
   * CUDA driver has been unloaded.
   */
  cudaErrorCudartUnloading              =     29,

  /**
   * This indicates that an unknown internal error has occurred.
   */
  cudaErrorUnknown                      =     30,

  /**
   * This indicates that the API call is not yet implemented. Production
   * releases of CUDA will never return this error.
   * \deprecated
   * This error return is deprecated as of CUDA 4.1.
   */
  cudaErrorNotYetImplemented            =     31,

  /**
   * This indicated that an emulated device pointer exceeded the 32-bit address
   * range.
   * \deprecated
   * This error return is deprecated as of CUDA 3.1. Device emulation mode was
   * removed with the CUDA 3.1 release.
   */
  cudaErrorMemoryValueTooLarge          =     32,

  /**
   * This indicates that a resource handle passed to the API call was not
   * valid. Resource handles are opaque types like ::cudaStream_t and
   * ::cudaEvent_t.
   */
  cudaErrorInvalidResourceHandle        =     33,

  /**
   * This indicates that asynchronous operations issued previously have not
   * completed yet. This result is not actually an error, but must be indicated
   * differently than ::cudaSuccess (which indicates completion). Calls that
   * may return this value include ::cudaEventQuery() and ::cudaStreamQuery().
   */
  cudaErrorNotReady                     =     34,

  /**
   * This indicates that the installed NVIDIA CUDA driver is older than the
   * CUDA runtime library. This is not a supported configuration. Users should
   * install an updated NVIDIA display driver to allow the application to run.
   */
  cudaErrorInsufficientDriver           =     35,

  /**
   * This indicates that the user has called ::cudaSetValidDevices(),
   * ::cudaSetDeviceFlags(), ::cudaD3D9SetDirect3DDevice(),
   * ::cudaD3D10SetDirect3DDevice, ::cudaD3D11SetDirect3DDevice(), or
   * ::cudaVDPAUSetVDPAUDevice() after initializing the CUDA runtime by
   * calling non-device management operations (allocating memory and
   * launching kernels are examples of non-device management operations).
   * This error can also be returned if using runtime/driver
   * interoperability and there is an existing ::CUcontext active on the
   * host thread.
   */
  cudaErrorSetOnActiveProcess           =     36,

  /**
   * This indicates that the surface passed to the API call is not a valid
   * surface.
   */
  cudaErrorInvalidSurface               =     37,

  /**
   * This indicates that no CUDA-capable devices were detected by the installed
   * CUDA driver.
   */
  cudaErrorNoDevice                     =     38,

  /**
   * This indicates that an uncorrectable ECC error was detected during
   * execution.
   */
  cudaErrorECCUncorrectable             =     39,

  /**
   * This indicates that a link to a shared object failed to resolve.
   */
  cudaErrorSharedObjectSymbolNotFound   =     40,

  /**
   * This indicates that initialization of a shared object failed.
   */
  cudaErrorSharedObjectInitFailed       =     41,

  /**
   * This indicates that the ::cudaLimit passed to the API call is not
   * supported by the active device.
   */
  cudaErrorUnsupportedLimit             =     42,

  /**
   * This indicates that multiple global or constant variables (across separate
   * CUDA source files in the application) share the same string name.
   */
  cudaErrorDuplicateVariableName        =     43,

  /**
   * This indicates that multiple textures (across separate CUDA source
   * files in the application) share the same string name.
   */
  cudaErrorDuplicateTextureName         =     44,

  /**
   * This indicates that multiple surfaces (across separate CUDA source
   * files in the application) share the same string name.
   */
  cudaErrorDuplicateSurfaceName         =     45,

  /**
   * This indicates that all CUDA devices are busy or unavailable at the current
   * time. Devices are often busy/unavailable due to use of
   * ::cudaComputeModeExclusive, ::cudaComputeModeProhibited or when long
   * running CUDA kernels have filled up the GPU and are blocking new work
   * from starting. They can also be unavailable due to memory constraints
   * on a device that already has active CUDA work being performed.
   */
  cudaErrorDevicesUnavailable           =     46,

  /**
   * This indicates that the device kernel image is invalid.
   */
  cudaErrorInvalidKernelImage           =     47,

  /**
   * This indicates that there is no kernel image available that is suitable
   * for the device. This can occur when a user specifies code generation
   * options for a particular CUDA source file that do not include the
   * corresponding device configuration.
   */
  cudaErrorNoKernelImageForDevice       =     48,

  /**
   * This indicates that the current context is not compatible with this
   * the CUDA Runtime. This can only occur if you are using CUDA
   * Runtime/Driver interoperability and have created an existing Driver
   * context using the driver API. The Driver context may be incompatible
   * either because the Driver context was created using an older version
   * of the API, because the Runtime API call expects a primary driver
   * context and the Driver context is not primary, or because the Driver
   * context has been destroyed. Please see \ref CUDART_DRIVER "Interactions
   * with the CUDA Driver API" for more information.
   */
  cudaErrorIncompatibleDriverContext    =     49,

  /**
   * This error indicates that a call to ::cudaDeviceEnablePeerAccess() is
   * trying to re-enable peer addressing on from a context which has already
   * had peer addressing enabled.
   */
  cudaErrorPeerAccessAlreadyEnabled     =     50,

  /**
   * This error indicates that ::cudaDeviceDisablePeerAccess() is trying to
   * disable peer addressing which has not been enabled yet via
   * ::cudaDeviceEnablePeerAccess().
   */
  cudaErrorPeerAccessNotEnabled         =     51,

  /**
   * This indicates that a call tried to access an exclusive-thread device that
   * is already in use by a different thread.
   */
  cudaErrorDeviceAlreadyInUse           =     54,

  /**
   * This indicates profiler is not initialized for this run. This can
   * happen when the application is running with external profiling tools
   * like visual profiler.
   */
  cudaErrorProfilerDisabled             =     55,

  /**
   * \deprecated
   * This error return is deprecated as of CUDA 5.0. It is no longer an error
   * to attempt to enable/disable the profiling via ::cudaProfilerStart or
   * ::cudaProfilerStop without initialization.
   */
  cudaErrorProfilerNotInitialized       =     56,

  /**
   * \deprecated
   * This error return is deprecated as of CUDA 5.0. It is no longer an error
   * to call cudaProfilerStart() when profiling is already enabled.
   */
  cudaErrorProfilerAlreadyStarted       =     57,

  /**
   * \deprecated
   * This error return is deprecated as of CUDA 5.0. It is no longer an error
   * to call cudaProfilerStop() when profiling is already disabled.
   */
   cudaErrorProfilerAlreadyStopped       =    58,

  /**
   * An assert triggered in device code during kernel execution. The device
   * cannot be used again. All existing allocations are invalid. To continue
   * using CUDA, the process must be terminated and relaunched.
   */
  cudaErrorAssert                        =    59,

  /**
   * This error indicates that the hardware resources required to enable
   * peer access have been exhausted for one or more of the devices
   * passed to ::cudaEnablePeerAccess().
   */
  cudaErrorTooManyPeers                 =     60,

  /**
   * This error indicates that the memory range passed to ::cudaHostRegister()
   * has already been registered.
   */
  cudaErrorHostMemoryAlreadyRegistered  =     61,

  /**
   * This error indicates that the pointer passed to ::cudaHostUnregister()
   * does not correspond to any currently registered memory region.
   */
  cudaErrorHostMemoryNotRegistered      =     62,

  /**
   * This error indicates that an OS call failed.
   */
  cudaErrorOperatingSystem              =     63,

  /**
   * This error indicates that P2P access is not supported across the given
   * devices.
   */
  cudaErrorPeerAccessUnsupported        =     64,

  /**
   * This error indicates that a device runtime grid launch did not occur
   * because the depth of the child grid would exceed the maximum supported
   * number of nested grid launches.
   */
  cudaErrorLaunchMaxDepthExceeded       =     65,

  /**
   * This error indicates that a grid launch did not occur because the kernel
   * uses file-scoped textures which are unsupported by the device runtime.
   * Kernels launched via the device runtime only support textures created with
   * the Texture Object API's.
   */
  cudaErrorLaunchFileScopedTex          =     66,

  /**
   * This error indicates that a grid launch did not occur because the kernel
   * uses file-scoped surfaces which are unsupported by the device runtime.
   * Kernels launched via the device runtime only support surfaces created with
   * the Surface Object API's.
   */
  cudaErrorLaunchFileScopedSurf         =     67,

  /**
   * This error indicates that a call to ::cudaDeviceSynchronize made from
   * the device runtime failed because the call was made at grid depth greater
   * than than either the default (2 levels of grids) or user specified device
   * limit ::cudaLimitDevRuntimeSyncDepth. To be able to synchronize on
   * launched grids at a greater depth successfully, the maximum nested
   * depth at which ::cudaDeviceSynchronize will be called must be specified
   * with the ::cudaLimitDevRuntimeSyncDepth limit to the ::cudaDeviceSetLimit
   * api before the host-side launch of a kernel using the device runtime.
   * Keep in mind that additional levels of sync depth require the runtime
   * to reserve large amounts of device memory that cannot be used for
   * user allocations.
   */
  cudaErrorSyncDepthExceeded            =     68,

  /**
   * This error indicates that a device runtime grid launch failed because
   * the launch would exceed the limit ::cudaLimitDevRuntimePendingLaunchCount.
   * For this launch to proceed successfully, ::cudaDeviceSetLimit must be
   * called to set the ::cudaLimitDevRuntimePendingLaunchCount to be higher
   * than the upper bound of outstanding launches that can be issued to the
   * device runtime. Keep in mind that raising the limit of pending device
   * runtime launches will require the runtime to reserve device memory that
   * cannot be used for user allocations.
   */
  cudaErrorLaunchPendingCountExceeded   =     69,

  /**
   * This error indicates the attempted operation is not permitted.
   */
  cudaErrorNotPermitted                 =     70,

  /**
   * This error indicates the attempted operation is not supported
   * on the current system or device.
   */
  cudaErrorNotSupported                 =     71,

  /**
   * Device encountered an error in the call stack during kernel execution,
   * possibly due to stack corruption or exceeding the stack size limit.
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorHardwareStackError           =     72,

  /**
   * The device encountered an illegal instruction during kernel execution
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorIllegalInstruction           =     73,

  /**
   * The device encountered a load or store instruction
   * on a memory address which is not aligned.
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorMisalignedAddress            =     74,

  /**
   * While executing a kernel, the device encountered an instruction
   * which can only operate on memory locations in certain address spaces
   * (global, shared, or local), but was supplied a memory address not
   * belonging to an allowed address space.
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorInvalidAddressSpace          =     75,

  /**
   * The device encountered an invalid program counter.
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorInvalidPc                    =     76,

  /**
   * The device encountered a load or store instruction on an invalid memory address.
   * This leaves the process in an inconsistent state and any further CUDA work
   * will return the same error. To continue using CUDA, the process must be terminated
   * and relaunched.
   */
  cudaErrorIllegalAddress               =     77,

  /**
   * A PTX compilation failed. The runtime may fall back to compiling PTX if
   * an application does not contain a suitable binary for the current device.
   */
  cudaErrorInvalidPtx                   =     78,

  /**
   * This indicates an error with the OpenGL or DirectX context.
   */
  cudaErrorInvalidGraphicsContext       =     79,

  /**
   * This indicates that an uncorrectable NVLink error was detected during the
   * execution.
   */
  cudaErrorNvlinkUncorrectable          =     80,

  /**
   * This indicates that the PTX JIT compiler library was not found. The JIT Compiler
   * library is used for PTX compilation. The runtime may fall back to compiling PTX
   * if an application does not contain a suitable binary for the current device.
   */
  cudaErrorJitCompilerNotFound          =     81,

  /**
   * This error indicates that the number of blocks launched per grid for a kernel that was
   * launched via either ::cudaLaunchCooperativeKernel or ::cudaLaunchCooperativeKernelMultiDevice
   * exceeds the maximum number of blocks as allowed by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor
   * or ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags times the number of multiprocessors
   * as specified by the device attribute ::cudaDevAttrMultiProcessorCount.
   */
  cudaErrorCooperativeLaunchTooLarge    =     82,

  /**
   * This error indicates that the system is not yet ready to start any CUDA
   * work.  To continue using CUDA, verify the system configuration is in a
   * valid state and all required driver daemons are actively running.
   */
  cudaErrorSystemNotReady               =     83,

  /**
   * This indicates that a resource required by the API call is not in a
   * valid state to perform the requested operation.
   */
  cudaErrorIllegalState                 =     84,

  /**
   * This indicates an internal startup failure in the CUDA runtime.
   */
  cudaErrorStartupFailure               =    127,

  /**
   * The operation is not permitted when the stream is capturing.
   */
  cudaErrorStreamCaptureUnsupported     =    900,

  /**
   * The current capture sequence on the stream has been invalidated due to
   * a previous error.
   */
  cudaErrorStreamCaptureInvalidated     =    901,

  /**
   * The operation would have resulted in a merge of two independent capture
   * sequences.
   */
  cudaErrorStreamCaptureMerge           =    902,

  /**
   * The capture was not initiated in this stream.
   */
  cudaErrorStreamCaptureUnmatched       =    903,

  /**
   * The capture sequence contains a fork that was not joined to the primary
   * stream.
   */
  cudaErrorStreamCaptureUnjoined        =    904,

  /**
   * A dependency would have been created which crosses the capture sequence
   * boundary. Only implicit in-stream ordering dependencies are allowed to
   * cross the boundary.
   */
  cudaErrorStreamCaptureIsolation       =    905,

  /**
   * The operation would have resulted in a disallowed implicit dependency on
   * a current capture sequence from cudaStreamLegacy.
   */
  cudaErrorStreamCaptureImplicit        =    906,

  /**
   * The operation is not permitted on an event which was last recorded in a
   * capturing stream.
   */
  cudaErrorCapturedEvent                =    907,

  /**
   * Any unhandled CUDA driver error is added to this value and returned via
   * the runtime. Production releases of CUDA should not return such errors.
   * \deprecated
   * This error return is deprecated as of CUDA 4.1.
   */
  cudaErrorApiFailureBase               =  10000
}

/**
 * Channel format kind
 */
alias cudaChannelFormatKind = int;
enum : cudaChannelFormatKind
{
  cudaChannelFormatKindSigned           =   0,
  cudaChannelFormatKindUnsigned         =   1,
  cudaChannelFormatKindFloat            =   2,
  cudaChannelFormatKindNone             =   3
}

/**
 * CUDA Channel format descriptor
 */
struct cudaChannelFormatDesc
{
  int                   x;
  int                   y;
  int                   z;
  int                   w;
  cudaChannelFormatKind f;
}

/**
 * CUDA array
 */
struct cudaArray;
alias cudaArray_t = void*;
alias cudaArray_const_t = const(void)*;
alias cudaMipmappedArray_t = void*;
alias cudaMipmappedArray_const_t = const(void)*;

alias cudaMemoryType = int;
enum : cudaMemoryType
{
cudaMemoryTypeUnregistered = 0, /**< Unregistered memory */
cudaMemoryTypeHost         = 1, /**< Host memory */
cudaMemoryTypeDevice       = 2, /**< Device memory */
cudaMemoryTypeManaged      = 3  /**< Managed memory */
}

alias cudaMemcpyKind = int;
enum : cudaMemcpyKind
{
  cudaMemcpyHostToHost          =   0,
  cudaMemcpyHostToDevice        =   1,
  cudaMemcpyDeviceToHost        =   2,
  cudaMemcpyDeviceToDevice      =   3,
  cudaMemcpyDefault             =   4
}

/**
 * CUDA Pitched memory pointer
 *
 * \sa ::make_cudaPitchedPtr
 */
struct cudaPitchedPtr
{
  void   *ptr;
  size_t  pitch;
  size_t  xsize;
  size_t  ysize;
}

/**
 * CUDA extent
 *
 * \sa ::make_cudaExtent
 */
struct cudaExtent
{
  size_t width;
  size_t height;
  size_t depth;
}

/**
 * CUDA 3D position
 *
 * \sa ::make_cudaPos
 */
struct cudaPos
{
  size_t x;
  size_t y;
  size_t z;
}

/**
 * CUDA 3D memory copying parameters
 */
struct cudaMemcpy3DParms
{
  cudaArray_t     srcArray;
  cudaPos         srcPos;
  cudaPitchedPtr  srcPtr;

  cudaArray_t     dstArray;
  cudaPos         dstPos;
  cudaPitchedPtr  dstPtr;

  cudaExtent      extent;
  cudaMemcpyKind  kind;
}

/**
 * CUDA 3D cross-device memory copying parameters
 */
struct cudaMemcpy3DPeerParms
{
  cudaArray_t     srcArray;
  cudaPos         srcPos;
  cudaPitchedPtr  srcPtr;
  int             srcDevice;
  cudaArray_t     dstArray;
  cudaPos         dstPos;
  cudaPitchedPtr  dstPtr;
  int             dstDevice;
  cudaExtent      extent;
}

/**
 * CUDA Memset node parameters
 */
struct cudaMemsetParams {
  void *dst;                              /**< Destination device pointer */
  size_t pitch;                           /**< Pitch of destination device pointer. Unused if height is 1 */
  uint value;                     /**< Value to be set */
  uint elementSize;               /**< Size of each element in bytes. Must be 1, 2, or 4. */
  size_t width;                           /**< Width in bytes, of the row */
  size_t height;                          /**< Number of rows */
}


/**
 * CUDA host function
 * \param userData Argument value passed to the function
 */
extern(System) nothrow
{
  alias cudaHostFn_t = void function(void *userData);
}


/**
 * CUDA host node parameters
 */
struct cudaHostNodeParams {
  cudaHostFn_t fn;    /**< The function to call when the node executes */
  void* userData; /**< Argument to pass to the function */
}

/**
 * Possible stream capture statuses returned by ::cudaStreamIsCapturing
 */
alias cudaStreamCaptureStatus = int;
enum : cudaStreamCaptureStatus {
  cudaStreamCaptureStatusNone        = 0, /**< Stream is not capturing */
  cudaStreamCaptureStatusActive      = 1, /**< Stream is actively capturing */
  cudaStreamCaptureStatusInvalidated = 2  /**< Stream is part of a capture sequence that
                                                   has been invalidated, but not terminated */
}

struct cudaGraphicsResource;

alias cudaGraphicsRegisterFlags = int;
enum : cudaGraphicsRegisterFlags{
  cudaGraphicsRegisterFlagsNone             = 0,
  cudaGraphicsRegisterFlagsReadOnly         = 1,
  cudaGraphicsRegisterFlagsWriteDiscard     = 2,
  cudaGraphicsRegisterFlagsSurfaceLoadStore = 4,
  cudaGraphicsRegisterFlagsTextureGather    = 8
}

alias cudaGraphicsMapFlags = int;
enum : cudaGraphicsMapFlags{
  cudaGraphicsMapFlagsNone         = 0,
  cudaGraphicsMapFlagsReadOnly     = 1,
  cudaGraphicsMapFlagsWriteDiscard = 2
}

alias cudaGraphicsCubeFace = int;
enum : cudaGraphicsCubeFace{
  cudaGraphicsCubeFacePositiveX = 0x00,
  cudaGraphicsCubeFaceNegativeX = 0x01,
  cudaGraphicsCubeFacePositiveY = 0x02,
  cudaGraphicsCubeFaceNegativeY = 0x03,
  cudaGraphicsCubeFacePositiveZ = 0x04,
  cudaGraphicsCubeFaceNegativeZ = 0x05
}

alias cudaResourceType = int;
enum : cudaResourceType{
  cudaResourceTypeArray          = 0x00,
  cudaResourceTypeMipmappedArray = 0x01,
  cudaResourceTypeLinear         = 0x02,
  cudaResourceTypePitch2D        = 0x03
}

alias cudaResourceViewFormat = int;
enum : cudaResourceViewFormat{
  cudaResViewFormatNone                      = 0x00,
  cudaResViewFormatUnsignedChar1             = 0x01,
  cudaResViewFormatUnsignedChar2             = 0x02,
  cudaResViewFormatUnsignedChar4             = 0x03,
  cudaResViewFormatSignedChar1               = 0x04,
  cudaResViewFormatSignedChar2               = 0x05,
  cudaResViewFormatSignedChar4               = 0x06,
  cudaResViewFormatUnsignedShort1            = 0x07,
  cudaResViewFormatUnsignedShort2            = 0x08,
  cudaResViewFormatUnsignedShort4            = 0x09,
  cudaResViewFormatSignedShort1              = 0x0a,
  cudaResViewFormatSignedShort2              = 0x0b,
  cudaResViewFormatSignedShort4              = 0x0c,
  cudaResViewFormatUnsignedInt1              = 0x0d,
  cudaResViewFormatUnsignedInt2              = 0x0e,
  cudaResViewFormatUnsignedInt4              = 0x0f,
  cudaResViewFormatSignedInt1                = 0x10,
  cudaResViewFormatSignedInt2                = 0x11,
  cudaResViewFormatSignedInt4                = 0x12,
  cudaResViewFormatHalf1                     = 0x13,
  cudaResViewFormatHalf2                     = 0x14,
  cudaResViewFormatHalf4                     = 0x15,
  cudaResViewFormatFloat1                    = 0x16,
  cudaResViewFormatFloat2                    = 0x17,
  cudaResViewFormatFloat4                    = 0x18,
  cudaResViewFormatUnsignedBlockCompressed1  = 0x19,
  cudaResViewFormatUnsignedBlockCompressed2  = 0x1a,
  cudaResViewFormatUnsignedBlockCompressed3  = 0x1b,
  cudaResViewFormatUnsignedBlockCompressed4  = 0x1c,
  cudaResViewFormatSignedBlockCompressed4    = 0x1d,
  cudaResViewFormatUnsignedBlockCompressed5  = 0x1e,
  cudaResViewFormatSignedBlockCompressed5    = 0x1f,
  cudaResViewFormatUnsignedBlockCompressed6H = 0x20,
  cudaResViewFormatSignedBlockCompressed6H   = 0x21,
  cudaResViewFormatUnsignedBlockCompressed7  = 0x22
}

/**
 * CUDA resource descriptor
 */
struct cudaResourceDesc
{
  cudaResourceType resType;

  union res_st
  {
    struct array_st
    {
      cudaArray_t array;
    }
    array_st array;

    struct mipmap_st
    {
      cudaMipmappedArray_t mipmap;
    }
    mipmap_st mipmap;

    struct linear_st
    {
      void *devPtr;
      cudaChannelFormatDesc desc;
      size_t sizeInBytes;
    }
    linear_st linear;

    struct pitch2D_st
    {
      void *devPtr;
      cudaChannelFormatDesc desc;
      size_t width;
      size_t height;
      size_t pitchInBytes;
    }
    pitch2D_st pitch2D;
  }
  res_st res;
}

/**
 * CUDA resource view descriptor
 */
struct cudaResourceViewDesc {
  cudaResourceViewFormat format;
  size_t                 width;
  size_t                 height;
  size_t                 depth;
  uint                   firstMipmapLevel;
  uint                   lastMipmapLevel;
  uint                   firstLayer;
  uint                   lastLayer;
}

/**
 * CUDA pointer attributes
 */
struct cudaPointerAttributes {
  cudaMemoryType memoryType;
  cudaMemoryType type;
  int device;
  void *devicePointer;
  void *hostPointer;
  int isManaged;
}

/**
 * CUDA function attributes
 */
struct cudaFuncAttributes {
 size_t sharedSizeBytes;
 size_t constSizeBytes;
 size_t localSizeBytes;
 int maxThreadsPerBlock;
 int numRegs;
 int ptxVersion;
 int binaryVersion;
 int cacheModeCA;
 int maxDynamicSharedSizeBytes;
 int preferredShmemCarveout;
}

alias cudaFuncAttribute = int;
enum : cudaFuncAttribute {
  cudaFuncAttributeMaxDynamicSharedMemorySize = 8, /**< Maximum dynamic shared memory size */
  cudaFuncAttributePreferredSharedMemoryCarveout = 9, /**< Preferred shared memory-L1 cache split ratio */
  cudaFuncAttributeMax
}

/**
 * CUDA function attributes that can be set using cudaFuncSetAttribute
 */
alias cudaFuncCache = int;
enum : cudaFuncCache {
  cudaFuncCachePreferNone   = 0,
  cudaFuncCachePreferShared = 1,
  cudaFuncCachePreferL1     = 2,
  cudaFuncCachePreferEqual  = 3
}

/**
 * CUDA shared memory configuration
 */
alias cudaSharedMemConfig = int;
enum : cudaSharedMemConfig {
  cudaSharedMemBankSizeDefault   = 0,
  cudaSharedMemBankSizeFourByte  = 1,
  cudaSharedMemBankSizeEightByte = 2
}

/**
 * Shared memory carveout configurations
 */
alias cudaSharedCarveout = int;
enum : cudaSharedCarveout {
  cudaSharedmemCarveoutDefault      = -1,  /* * < no preference for shared memory or L1 (default) */
  cudaSharedmemCarveoutMaxShared    = 100, /* * < prefer maximum available shared memory, minimum L1 cache */
  cudaSharedmemCarveoutMaxL1        = 0    /* * < prefer maximum available L1 cache, minimum shared memory */
}

/**
 * CUDA device compute modes
 */
alias cudaComputeMode = int;
enum : cudaComputeMode {
  cudaComputeModeDefault          = 0,
  cudaComputeModeExclusive        = 1,
  cudaComputeModeProhibited       = 2,
  cudaComputeModeExclusiveProcess = 3
}


/**
 * CUDA Limits
 */
alias cudaLimit = int;
enum : cudaLimit {
  cudaLimitStackSize                    = 0x00,
  cudaLimitPrintfFifoSize               = 0x01,
  cudaLimitMallocHeapSize               = 0x02,
  cudaLimitDevRuntimeSyncDepth          = 0x03,
  cudaLimitDevRuntimePendingLaunchCount = 0x04,
  cudaLimitMaxL2FetchGranularity        = 0x05
}

/**
 * CUDA Memory Advise values
 */
alias cudaMemoryAdvise = int;
enum : cudaMemoryAdvise {
  cudaMemAdviseSetReadMostly          = 1, /**< Data will mostly be read and only occassionally be written to */
  cudaMemAdviseUnsetReadMostly        = 2, /**< Undo the effect of ::cudaMemAdviseSetReadMostly */
  cudaMemAdviseSetPreferredLocation   = 3, /**< Set the preferred location for the data as the specified device */
  cudaMemAdviseUnsetPreferredLocation = 4, /**< Clear the preferred location for the data */
  cudaMemAdviseSetAccessedBy          = 5, /**< Data will be accessed by the specified device, so prevent page faults as much as possible */
  cudaMemAdviseUnsetAccessedBy        = 6  /**< Let the Unified Memory subsystem decide on the page faulting policy for the specified device */
}

/**
 * CUDA range attributes
 */
alias cudaMemRangeAttribute = int;
enum : cudaMemRangeAttribute {
  cudaMemRangeAttributeReadMostly           = 1, /**< Whether the range will mostly be read and only occassionally be written to */
  cudaMemRangeAttributePreferredLocation    = 2, /**< The preferred location of the range */
  cudaMemRangeAttributeAccessedBy           = 3, /**< Memory range has ::cudaMemAdviseSetAccessedBy set for specified device */
  cudaMemRangeAttributeLastPrefetchLocation = 4  /**< The last location to which the range was prefetched */
}

/**
 * CUDA Profiler Output modes
 */
alias cudaOutputMode = int;
enum : cudaOutputMode {
  cudaKeyValuePair    = 0x00,
  cudaCSV             = 0x01
}

/**
 * CUDA device attributes
 */
alias cudaDeviceAttr = int;
enum : cudaDeviceAttr {
  cudaDevAttrMaxThreadsPerBlock             = 1,  /**< Maximum number of threads per block */
  cudaDevAttrMaxBlockDimX                   = 2,  /**< Maximum block dimension X */
  cudaDevAttrMaxBlockDimY                   = 3,  /**< Maximum block dimension Y */
  cudaDevAttrMaxBlockDimZ                   = 4,  /**< Maximum block dimension Z */
  cudaDevAttrMaxGridDimX                    = 5,  /**< Maximum grid dimension X */
  cudaDevAttrMaxGridDimY                    = 6,  /**< Maximum grid dimension Y */
  cudaDevAttrMaxGridDimZ                    = 7,  /**< Maximum grid dimension Z */
  cudaDevAttrMaxSharedMemoryPerBlock        = 8,  /**< Maximum shared memory available per block in bytes */
  cudaDevAttrTotalConstantMemory            = 9,  /**< Memory available on device for __constant__ variables in a CUDA C kernel in bytes */
  cudaDevAttrWarpSize                       = 10, /**< Warp size in threads */
  cudaDevAttrMaxPitch                       = 11, /**< Maximum pitch in bytes allowed by memory copies */
  cudaDevAttrMaxRegistersPerBlock           = 12, /**< Maximum number of 32-bit registers available per block */
  cudaDevAttrClockRate                      = 13, /**< Peak clock frequency in kilohertz */
  cudaDevAttrTextureAlignment               = 14, /**< Alignment requirement for textures */
  cudaDevAttrGpuOverlap                     = 15, /**< Device can possibly copy memory and execute a kernel concurrently */
  cudaDevAttrMultiProcessorCount            = 16, /**< Number of multiprocessors on device */
  cudaDevAttrKernelExecTimeout              = 17, /**< Specifies whether there is a run time limit on kernels */
  cudaDevAttrIntegrated                     = 18, /**< Device is integrated with host memory */
  cudaDevAttrCanMapHostMemory               = 19, /**< Device can map host memory into CUDA address space */
  cudaDevAttrComputeMode                    = 20, /**< Compute mode (See ::cudaComputeMode for details) */
  cudaDevAttrMaxTexture1DWidth              = 21, /**< Maximum 1D texture width */
  cudaDevAttrMaxTexture2DWidth              = 22, /**< Maximum 2D texture width */
  cudaDevAttrMaxTexture2DHeight             = 23, /**< Maximum 2D texture height */
  cudaDevAttrMaxTexture3DWidth              = 24, /**< Maximum 3D texture width */
  cudaDevAttrMaxTexture3DHeight             = 25, /**< Maximum 3D texture height */
  cudaDevAttrMaxTexture3DDepth              = 26, /**< Maximum 3D texture depth */
  cudaDevAttrMaxTexture2DLayeredWidth       = 27, /**< Maximum 2D layered texture width */
  cudaDevAttrMaxTexture2DLayeredHeight      = 28, /**< Maximum 2D layered texture height */
  cudaDevAttrMaxTexture2DLayeredLayers      = 29, /**< Maximum layers in a 2D layered texture */
  cudaDevAttrSurfaceAlignment               = 30, /**< Alignment requirement for surfaces */
  cudaDevAttrConcurrentKernels              = 31, /**< Device can possibly execute multiple kernels concurrently */
  cudaDevAttrEccEnabled                     = 32, /**< Device has ECC support enabled */
  cudaDevAttrPciBusId                       = 33, /**< PCI bus ID of the device */
  cudaDevAttrPciDeviceId                    = 34, /**< PCI device ID of the device */
  cudaDevAttrTccDriver                      = 35, /**< Device is using TCC driver model */
  cudaDevAttrMemoryClockRate                = 36, /**< Peak memory clock frequency in kilohertz */
  cudaDevAttrGlobalMemoryBusWidth           = 37, /**< Global memory bus width in bits */
  cudaDevAttrL2CacheSize                    = 38, /**< Size of L2 cache in bytes */
  cudaDevAttrMaxThreadsPerMultiProcessor    = 39, /**< Maximum resident threads per multiprocessor */
  cudaDevAttrAsyncEngineCount               = 40, /**< Number of asynchronous engines */
  cudaDevAttrUnifiedAddressing              = 41, /**< Device shares a unified address space with the host */
  cudaDevAttrMaxTexture1DLayeredWidth       = 42, /**< Maximum 1D layered texture width */
  cudaDevAttrMaxTexture1DLayeredLayers      = 43, /**< Maximum layers in a 1D layered texture */
  cudaDevAttrMaxTexture2DGatherWidth        = 45, /**< Maximum 2D texture width if cudaArrayTextureGather is set */
  cudaDevAttrMaxTexture2DGatherHeight       = 46, /**< Maximum 2D texture height if cudaArrayTextureGather is set */
  cudaDevAttrMaxTexture3DWidthAlt           = 47, /**< Alternate maximum 3D texture width */
  cudaDevAttrMaxTexture3DHeightAlt          = 48, /**< Alternate maximum 3D texture height */
  cudaDevAttrMaxTexture3DDepthAlt           = 49, /**< Alternate maximum 3D texture depth */
  cudaDevAttrPciDomainId                    = 50, /**< PCI domain ID of the device */
  cudaDevAttrTexturePitchAlignment          = 51, /**< Pitch alignment requirement for textures */
  cudaDevAttrMaxTextureCubemapWidth         = 52, /**< Maximum cubemap texture width/height */
  cudaDevAttrMaxTextureCubemapLayeredWidth  = 53, /**< Maximum cubemap layered texture width/height */
  cudaDevAttrMaxTextureCubemapLayeredLayers = 54, /**< Maximum layers in a cubemap layered texture */
  cudaDevAttrMaxSurface1DWidth              = 55, /**< Maximum 1D surface width */
  cudaDevAttrMaxSurface2DWidth              = 56, /**< Maximum 2D surface width */
  cudaDevAttrMaxSurface2DHeight             = 57, /**< Maximum 2D surface height */
  cudaDevAttrMaxSurface3DWidth              = 58, /**< Maximum 3D surface width */
  cudaDevAttrMaxSurface3DHeight             = 59, /**< Maximum 3D surface height */
  cudaDevAttrMaxSurface3DDepth              = 60, /**< Maximum 3D surface depth */
  cudaDevAttrMaxSurface1DLayeredWidth       = 61, /**< Maximum 1D layered surface width */
  cudaDevAttrMaxSurface1DLayeredLayers      = 62, /**< Maximum layers in a 1D layered surface */
  cudaDevAttrMaxSurface2DLayeredWidth       = 63, /**< Maximum 2D layered surface width */
  cudaDevAttrMaxSurface2DLayeredHeight      = 64, /**< Maximum 2D layered surface height */
  cudaDevAttrMaxSurface2DLayeredLayers      = 65, /**< Maximum layers in a 2D layered surface */
  cudaDevAttrMaxSurfaceCubemapWidth         = 66, /**< Maximum cubemap surface width */
  cudaDevAttrMaxSurfaceCubemapLayeredWidth  = 67, /**< Maximum cubemap layered surface width */
  cudaDevAttrMaxSurfaceCubemapLayeredLayers = 68, /**< Maximum layers in a cubemap layered surface */
  cudaDevAttrMaxTexture1DLinearWidth        = 69, /**< Maximum 1D linear texture width */
  cudaDevAttrMaxTexture2DLinearWidth        = 70, /**< Maximum 2D linear texture width */
  cudaDevAttrMaxTexture2DLinearHeight       = 71, /**< Maximum 2D linear texture height */
  cudaDevAttrMaxTexture2DLinearPitch        = 72, /**< Maximum 2D linear texture pitch in bytes */
  cudaDevAttrMaxTexture2DMipmappedWidth     = 73, /**< Maximum mipmapped 2D texture width */
  cudaDevAttrMaxTexture2DMipmappedHeight    = 74, /**< Maximum mipmapped 2D texture height */
  cudaDevAttrComputeCapabilityMajor         = 75, /**< Major compute capability version number */
  cudaDevAttrComputeCapabilityMinor         = 76, /**< Minor compute capability version number */
  cudaDevAttrMaxTexture1DMipmappedWidth     = 77, /**< Maximum mipmapped 1D texture width */
  cudaDevAttrStreamPrioritiesSupported      = 78, /**< Device supports stream priorities */
  cudaDevAttrGlobalL1CacheSupported         = 79, /**< Device supports caching globals in L1 */
  cudaDevAttrLocalL1CacheSupported          = 80, /**< Device supports caching locals in L1 */
  cudaDevAttrMaxSharedMemoryPerMultiprocessor = 81, /**< Maximum shared memory available per multiprocessor in bytes */
  cudaDevAttrMaxRegistersPerMultiprocessor  = 82, /**< Maximum number of 32-bit registers available per multiprocessor */
  cudaDevAttrManagedMemory                  = 83, /**< Device can allocate managed memory on this system */
  cudaDevAttrIsMultiGpuBoard                = 84, /**< Device is on a multi-GPU board */
  cudaDevAttrMultiGpuBoardGroupID           = 85, /**< Unique identifier for a group of devices on the same multi-GPU board */
  cudaDevAttrHostNativeAtomicSupported      = 86, /**< Link between the device and the host supports native atomic operations */
  cudaDevAttrSingleToDoublePrecisionPerfRatio = 87, /**< Ratio of single precision performance (in floating-point operations per second) to double precision performance */
  cudaDevAttrPageableMemoryAccess           = 88, /**< Device supports coherently accessing pageable memory without calling cudaHostRegister on it */
  cudaDevAttrConcurrentManagedAccess        = 89, /**< Device can coherently access managed memory concurrently with the CPU */
  cudaDevAttrComputePreemptionSupported     = 90, /**< Device supports Compute Preemption */
  cudaDevAttrCanUseHostPointerForRegisteredMem = 91, /**< Device can access host registered memory at the same virtual address as the CPU */
  cudaDevAttrReserved92                     = 92,
  cudaDevAttrReserved93                     = 93,
  cudaDevAttrReserved94                     = 94,
  cudaDevAttrCooperativeLaunch              = 95, /**< Device supports launching cooperative kernels via ::cudaLaunchCooperativeKernel*/
  cudaDevAttrCooperativeMultiDeviceLaunch   = 96, /**< Device can participate in cooperative kernels launched via ::cudaLaunchCooperativeKernelMultiDevice */
  cudaDevAttrMaxSharedMemoryPerBlockOptin   = 97, /**< The maximum optin shared memory per block. This value may vary by chip. See ::cudaFuncSetAttribute */
  cudaDevAttrCanFlushRemoteWrites           = 98, /**< Device supports flushing of outstanding remote writes. */
  cudaDevAttrHostRegisterSupported          = 99, /**< Device supports host memory registration via ::cudaHostRegister. */
  cudaDevAttrPageableMemoryAccessUsesHostPageTables = 100, /**< Device accesses pageable memory via the host's page tables. */
  cudaDevAttrDirectManagedMemAccessFromHost = 101 /**< Host can directly access managed memory on the device without migration. */
}

/**
 * CUDA device P2P attributes
 */
alias cudaDeviceP2PAttr = int;
enum : cudaDeviceP2PAttr {
  cudaDevP2PAttrPerformanceRank              = 1, /**< A relative value indicating the performance of the link between two devices */
  cudaDevP2PAttrAccessSupported              = 2, /**< Peer access is enabled */
  cudaDevP2PAttrNativeAtomicSupported        = 3, /**< Native atomic operation over the link supported */
  cudaDevP2PAttrCudaArrayAccessSupported     = 4  /**< Accessing CUDA arrays over the link supported */
}

/**
 * CUDA UUID types
 */
struct CUuuid_st {
  char[16] bytes;
}
alias cudaUUID_t = CUuuid_st;

/**
 * CUDA device properties
 */
struct cudaDeviceProp
{
  char[256]    name;                  /**< ASCII string identifying device */
  cudaUUID_t   uuid;                       /**< 16-byte unique identifier */
  char[8]      luid;                    /**< 8-byte locally unique identifier. Value is undefined on TCC and non-Windows platforms */
  uint         luidDeviceNodeMask;         /**< LUID device node mask. Value is undefined on TCC and non-Windows platforms */
  size_t       totalGlobalMem;             /**< Global memory available on device in bytes */
  size_t       sharedMemPerBlock;          /**< Shared memory available per block in bytes */
  int          regsPerBlock;               /**< 32-bit registers available per block */
  int          warpSize;                   /**< Warp size in threads */
  size_t       memPitch;                   /**< Maximum pitch in bytes allowed by memory copies */
  int          maxThreadsPerBlock;         /**< Maximum number of threads per block */
  int[3]       maxThreadsDim;           /**< Maximum size of each dimension of a block */
  int[3]          maxGridSize;             /**< Maximum size of each dimension of a grid */
  int          clockRate;                  /**< Clock frequency in kilohertz */
  size_t       totalConstMem;              /**< Constant memory available on device in bytes */
  int          major;                      /**< Major compute capability */
  int          minor;                      /**< Minor compute capability */
  size_t       textureAlignment;           /**< Alignment requirement for textures */
  size_t       texturePitchAlignment;      /**< Pitch alignment requirement for texture references bound to pitched memory */
  int          deviceOverlap;              /**< Device can concurrently copy memory and execute a kernel. Deprecated. Use instead asyncEngineCount. */
  int          multiProcessorCount;        /**< Number of multiprocessors on device */
  int          kernelExecTimeoutEnabled;   /**< Specified whether there is a run time limit on kernels */
  int          integrated;                 /**< Device is integrated as opposed to discrete */
  int          canMapHostMemory;           /**< Device can map host memory with cudaHostAlloc/cudaHostGetDevicePointer */
  int          computeMode;                /**< Compute mode (See ::cudaComputeMode) */
  int          maxTexture1D;               /**< Maximum 1D texture size */
  int          maxTexture1DMipmap;         /**< Maximum 1D mipmapped texture size */
  int          maxTexture1DLinear;         /**< Maximum size for 1D textures bound to linear memory */
  int[2]          maxTexture2D;            /**< Maximum 2D texture dimensions */
  int[2]          maxTexture2DMipmap;      /**< Maximum 2D mipmapped texture dimensions */
  int[3]          maxTexture2DLinear;      /**< Maximum dimensions (width, height, pitch) for 2D textures bound to pitched memory */
  int[2]          maxTexture2DGather;      /**< Maximum 2D texture dimensions if texture gather operations have to be performed */
  int[3]          maxTexture3D;            /**< Maximum 3D texture dimensions */
  int[3]          maxTexture3DAlt;         /**< Maximum alternate 3D texture dimensions */
  int          maxTextureCubemap;          /**< Maximum Cubemap texture dimensions */
  int[2]          maxTexture1DLayered;     /**< Maximum 1D layered texture dimensions */
  int[3]          maxTexture2DLayered;     /**< Maximum 2D layered texture dimensions */
  int[2]          maxTextureCubemapLayered;/**< Maximum Cubemap layered texture dimensions */
  int          maxSurface1D;               /**< Maximum 1D surface size */
  int[2]          maxSurface2D;            /**< Maximum 2D surface dimensions */
  int[3]          maxSurface3D;            /**< Maximum 3D surface dimensions */
  int[2]          maxSurface1DLayered;     /**< Maximum 1D layered surface dimensions */
  int[3]          maxSurface2DLayered;     /**< Maximum 2D layered surface dimensions */
  int          maxSurfaceCubemap;          /**< Maximum Cubemap surface dimensions */
  int[2]          maxSurfaceCubemapLayered;/**< Maximum Cubemap layered surface dimensions */
  size_t       surfaceAlignment;           /**< Alignment requirements for surfaces */
  int          concurrentKernels;          /**< Device can possibly execute multiple kernels concurrently */
  int          ECCEnabled;                 /**< Device has ECC support enabled */
  int          pciBusID;                   /**< PCI bus ID of the device */
  int          pciDeviceID;                /**< PCI device ID of the device */
  int          pciDomainID;                /**< PCI domain ID of the device */
  int          tccDriver;                  /**< 1 if device is a Tesla device using TCC driver, 0 otherwise */
  int          asyncEngineCount;           /**< Number of asynchronous engines */
  int          unifiedAddressing;          /**< Device shares a unified address space with the host */
  int          memoryClockRate;            /**< Peak memory clock frequency in kilohertz */
  int          memoryBusWidth;             /**< Global memory bus width in bits */
  int          l2CacheSize;                /**< Size of L2 cache in bytes */
  int          maxThreadsPerMultiProcessor;/**< Maximum resident threads per multiprocessor */
  int          streamPrioritiesSupported;  /**< Device supports stream priorities */
  int          globalL1CacheSupported;     /**< Device supports caching globals in L1 */
  int          localL1CacheSupported;      /**< Device supports caching locals in L1 */
  size_t       sharedMemPerMultiprocessor; /**< Shared memory available per multiprocessor in bytes */
  int          regsPerMultiprocessor;      /**< 32-bit registers available per multiprocessor */
  int          managedMemory;              /**< Device supports allocating managed memory on this system */
  int          isMultiGpuBoard;            /**< Device is on a multi-GPU board */
  int          multiGpuBoardGroupID;       /**< Unique identifier for a group of devices on the same multi-GPU board */
  int          hostNativeAtomicSupported;  /**< Link between the device and the host supports native atomic operations */
  int          singleToDoublePrecisionPerfRatio; /**< Ratio of single precision performance (in floating-point operations per second) to double precision performance */
  int          pageableMemoryAccess;       /**< Device supports coherently accessing pageable memory without calling cudaHostRegister on it */
  int          concurrentManagedAccess;    /**< Device can coherently access managed memory concurrently with the CPU */
  int          computePreemptionSupported; /**< Device supports Compute Preemption */
  int          canUseHostPointerForRegisteredMem; /**< Device can access host registered memory at the same virtual address as the CPU */
  int          cooperativeLaunch;          /**< Device supports launching cooperative kernels via ::cudaLaunchCooperativeKernel */
  int          cooperativeMultiDeviceLaunch; /**< Device can participate in cooperative kernels launched via ::cudaLaunchCooperativeKernelMultiDevice */
  size_t       sharedMemPerBlockOptin;     /**< Per device maximum shared memory per block usable by special opt in */
  int          pageableMemoryAccessUsesHostPageTables; /**< Device accesses pageable memory via the host's page tables */
  int          directManagedMemAccessFromHost; /**< Host can directly access managed memory on the device without migration. */
}

static immutable cudaDeviceProp cudaDevicePropDontCare = cudaDeviceProp(
  "\0",    /* char         name[256];               */
  cudaUUID_t(),     /* cudaUUID_t   uuid;                    */
  "\0",    /* char         luid[8];                 */
  0,         /* uint luidDeviceNodeMask       */
  0,         /* size_t       totalGlobalMem;          */
  0,         /* size_t       sharedMemPerBlock;       */
  0,         /* int          regsPerBlock;            */
  0,         /* int          warpSize;                */
  0,         /* size_t       memPitch;                */
  0,         /* int          maxThreadsPerBlock;      */
  [0, 0, 0], /* int          maxThreadsDim[3];        */
  [0, 0, 0], /* int          maxGridSize[3];          */
  0,         /* int          clockRate;               */
  0,         /* size_t       totalConstMem;           */
  -1,        /* int          major;                   */
  -1,        /* int          minor;                   */
  0,         /* size_t       textureAlignment;        */
  0,         /* size_t       texturePitchAlignment    */
  -1,        /* int          deviceOverlap;           */
  0,         /* int          multiProcessorCount;     */
  0,         /* int          kernelExecTimeoutEnabled */
  0,         /* int          integrated               */
  0,         /* int          canMapHostMemory         */
  0,         /* int          computeMode              */
  0,         /* int          maxTexture1D             */
  0,         /* int          maxTexture1DMipmap       */
  0,         /* int          maxTexture1DLinear       */
  [0, 0],    /* int          maxTexture2D[2]          */
  [0, 0],    /* int          maxTexture2DMipmap[2]    */
  [0, 0, 0], /* int          maxTexture2DLinear[3]    */
  [0, 0],    /* int          maxTexture2DGather[2]    */
  [0, 0, 0], /* int          maxTexture3D[3]          */
  [0, 0, 0], /* int          maxTexture3DAlt[3]       */
  0,         /* int          maxTextureCubemap        */
  [0, 0],    /* int          maxTexture1DLayered[2]   */
  [0, 0, 0], /* int          maxTexture2DLayered[3]   */
  [0, 0],    /* int          maxTextureCubemapLayered[2] */
  0,         /* int          maxSurface1D             */
  [0, 0],    /* int          maxSurface2D[2]          */
  [0, 0, 0], /* int          maxSurface3D[3]          */
  [0, 0],    /* int          maxSurface1DLayered[2]   */
  [0, 0, 0], /* int          maxSurface2DLayered[3]   */
  0,         /* int          maxSurfaceCubemap        */
  [0, 0],    /* int          maxSurfaceCubemapLayered[2] */
  0,         /* size_t       surfaceAlignment         */
  0,         /* int          concurrentKernels        */
  0,         /* int          ECCEnabled               */
  0,         /* int          pciBusID                 */
  0,         /* int          pciDeviceID              */
  0,         /* int          pciDomainID              */
  0,         /* int          tccDriver                */
  0,         /* int          asyncEngineCount         */
  0,         /* int          unifiedAddressing        */
  0,         /* int          memoryClockRate          */
  0,         /* int          memoryBusWidth           */
  0,         /* int          l2CacheSize              */
  0,         /* int          maxThreadsPerMultiProcessor */
  0,         /* int          streamPrioritiesSupported */
  0,         /* int          globalL1CacheSupported   */
  0,         /* int          localL1CacheSupported    */
  0,         /* size_t       sharedMemPerMultiprocessor; */
  0,         /* int          regsPerMultiprocessor;   */
  0,         /* int          managedMemory            */
  0,         /* int          isMultiGpuBoard          */
  0,         /* int          multiGpuBoardGroupID     */
  0,         /* int          hostNativeAtomicSupported */
  0,         /* int          singleToDoublePrecisionPerfRatio */
  0,         /* int          pageableMemoryAccess     */
  0,         /* int          concurrentManagedAccess  */
  0,         /* int          computePreemptionSupported */
  0,         /* int          canUseHostPointerForRegisteredMem */
  0,         /* int          cooperativeLaunch */
  0,         /* int          cooperativeMultiDeviceLaunch */
  0,         /* size_t       sharedMemPerBlockOptin */
  0,         /* int          pageableMemoryAccessUsesHostPageTables */
  0,         /* int          directManagedMemAccessFromHost */
);

enum CUDA_IPC_HANDLE_SIZE = 64;

struct cudaIpcEventHandle_t
{
  char[CUDA_IPC_HANDLE_SIZE] reserved;
}

struct cudaIpcMemHandle_t
{
  char[CUDA_IPC_HANDLE_SIZE] reserved;
}


/**
 * External memory handle types
 */
alias cudaExternalMemoryHandleType = int;
enum : cudaExternalMemoryHandleType {
  /**
   * Handle is an opaque file descriptor
   */
  cudaExternalMemoryHandleTypeOpaqueFd       = 1,
  /**
   * Handle is an opaque shared NT handle
   */
  cudaExternalMemoryHandleTypeOpaqueWin32    = 2,
  /**
   * Handle is an opaque, globally shared handle
   */
  cudaExternalMemoryHandleTypeOpaqueWin32Kmt = 3,
  /**
   * Handle is a D3D12 heap object
   */
  cudaExternalMemoryHandleTypeD3D12Heap      = 4,
  /**
   * Handle is a D3D12 committed resource
   */
  cudaExternalMemoryHandleTypeD3D12Resource  = 5
}

enum cudaExternalMemoryDedicated = 0x1;

// TODO(naetherm): driver_types.h:1747
struct cudaExternalMemoryHandleDesc {
  cudaExternalMemoryHandleType type;

  union handle_st {
    int fd;

    struct win32_st {
      void * handle;
      const void * name;
    }

    win32_st win32;
  }

  handle_st handle;
  /**
   * Size of the memory allocation
   */
  ulong size;
  /**
   * Flags must either be zero or ::cudaExternalMemoryDedicated
   */
  uint flags;
}


/**
 * External memory buffer descriptor
 */
struct cudaExternalMemoryBufferDesc {
  /**
   * Offset into the memory object where the buffer's base is
   */
  ulong offset;
  /**
   * Size of the buffer
   */
  ulong size;
  /**
   * Flags reserved for future use. Must be zero.
   */
  uint flags;
}

/**
 * External memory mipmap descriptor
 */
struct cudaExternalMemoryMipmappedArrayDesc {
  /**
   * Offset into the memory object where the base level of the
   * mipmap chain is.
   */
  long offset;
  /**
   * Format of base level of the mipmap chain
   */
  cudaChannelFormatDesc formatDesc;
  /**
   * Dimensions of base level of the mipmap chain
   */
  cudaExtent extent;
  /**
   * Flags associated with CUDA mipmapped arrays.
   * See ::cudaMallocMipmappedArray
   */
  uint flags;
  /**
   * Total number of levels in the mipmap chain
   */
  uint numLevels;
}

/**
 * External semaphore handle types
 */
alias cudaExternalSemaphoreHandleType = int;
enum : cudaExternalSemaphoreHandleType {
  /**
   * Handle is an opaque file descriptor
   */
  cudaExternalSemaphoreHandleTypeOpaqueFd       = 1,
  /**
   * Handle is an opaque shared NT handle
   */
  cudaExternalSemaphoreHandleTypeOpaqueWin32    = 2,
  /**
   * Handle is an opaque, globally shared handle
   */
  cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt = 3,
  /**
   * Handle is a shared NT handle referencing a D3D12 fence object
   */
  cudaExternalSemaphoreHandleTypeD3D12Fence     = 4
}



/**
 * External semaphore handle descriptor
 */
struct cudaExternalSemaphoreHandleDesc {
  /**
   * Type of the handle
   */
  cudaExternalSemaphoreHandleType type;
  union handle_st {
    /**
     * File descriptor referencing the semaphore object. Valid
     * when type is ::cudaExternalSemaphoreHandleTypeOpaqueFd
     */
    int fd;
    /**
     * Win32 handle referencing the semaphore object. Valid when
     * type is one of the following:
     * - ::cudaExternalSemaphoreHandleTypeOpaqueWin32
     * - ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
     * - ::cudaExternalSemaphoreHandleTypeD3D12Fence
     * Exactly one of 'handle' and 'name' must be non-NULL. If
     * type is ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
     * then 'name' must be NULL.
     */
    struct win32_st {
      /**
       * Valid NT handle. Must be NULL if 'name' is non-NULL
       */
      void *handle;
      /**
       * Name of a valid synchronization primitive.
       * Must be NULL if 'handle' is non-NULL.
       */
      const void *name;
    }

    win32_st win32;
  }

  handle_st handle;
  /**
   * Flags reserved for the future. Must be zero.
   */
  uint flags;
}

/**
 * External semaphore  signal parameters
 */
struct cudaExternalSemaphoreSignalParams {
  union params_st {
    /**
     * Parameters for fence objects
     */
    struct fence_st {
      /**
       * Value of fence to be signaled
       */
      ulong value;
    }

    fence_st fence;
  }

  params_st params;
  /**
   * Flags reserved for the future. Must be zero.
   */
  uint flags;
}

/**
* External semaphore wait parameters
*/
struct cudaExternalSemaphoreWaitParams {
  union params_st {
    /**
    * Parameters for fence objects
    */
    struct fence_st {
      /**
      * Value of fence to be waited on
      */
      ulong value;
    }
    fence_st fence;
  }
  params_st params;
  /**
  * Flags reserved for the future. Must be zero.
  */
  uint flags;
}

alias cudaError_t = cudaError;
alias cudaStream_t = void*;
alias cudaEvent_t = void*;
alias cudaGraphicsResource_t = void*;
alias cudaOutputMode_t = cudaOutputMode;
alias cudaExternalMemory_t = void*;
alias cudaExternalSemaphore_t = void*;
alias cudaGraph_t = void*;
alias cudaGraphNode_t = void*;


/**
 * CUDA cooperative group scope
 */
alias cudaCGScope = int;
enum : cudaCGScope {
  cudaCGScopeInvalid   = 0, /**< Invalid cooperative group scope */
  cudaCGScopeGrid      = 1, /**< Scope represented by a grid_group */
  cudaCGScopeMultiGrid = 2  /**< Scope represented by a multi_grid_group */
}

/**
 * CUDA launch parameters
 */
struct cudaLaunchParams {
  void *func;          /**< Device function symbol */
  dim3 gridDim;        /**< Grid dimentions */
  dim3 blockDim;       /**< Block dimentions */
  void **args;         /**< Arguments */
  size_t sharedMem;    /**< Shared memory */
  cudaStream_t stream; /**< Stream identifier */
}

/**
 * CUDA GPU kernel node parameters
 */
struct cudaKernelNodeParams {
  void* func;                     /**< Kernel to launch */
  dim3 gridDim;                   /**< Grid dimensions */
  dim3 blockDim;                  /**< Block dimensions */
  uint sharedMemBytes;    /**< Dynamic shared-memory size per thread block in bytes */
  void **kernelParams;            /**< Array of pointers to individual kernel arguments*/
  void **extra;                   /**< Pointer to kernel arguments in the "extra" format */
}

/**
* CUDA Graph node types
*/
alias cudaGraphNodeType = int;
enum : cudaGraphNodeType {
  cudaGraphNodeTypeKernel  = 0x00, /**< GPU kernel node */
  cudaGraphNodeTypeMemcpy  = 0x01, /**< Memcpy node */
  cudaGraphNodeTypeMemset  = 0x02, /**< Memset node */
  cudaGraphNodeTypeHost    = 0x03, /**< Host (executable) node */
  cudaGraphNodeTypeGraph   = 0x04, /**< Node which executes an embedded graph */
  cudaGraphNodeTypeEmpty   = 0x05, /**< Empty (no-op) node */
  cudaGraphNodeTypeCount
}

/**
 * CUDA executable (launchable) graph
 */
struct CUgraphExec_st;
alias cudaGraphExec_t = CUgraphExec_st*;


// surface_types.h

enum cudaSurfaceType1D              = 0x01;
enum cudaSurfaceType2D              = 0x02;
enum cudaSurfaceType3D              = 0x03;
enum cudaSurfaceTypeCubemap         = 0x0C;
enum cudaSurfaceType1DLayered       = 0xF1;
enum cudaSurfaceType2DLayered       = 0xF2;
enum cudaSurfaceTypeCubemapLayered  = 0xFC;

/**
 * CUDA Surface boundary modes
 */
alias cudaSurfaceBoundaryMode = int;
enum : cudaSurfaceBoundaryMode {
  cudaBoundaryModeZero  = 0,
  cudaBoundaryModeClamp = 1,
  cudaBoundaryModeTrap  = 2
}

/**
 * CUDA Surface format modes
 */
alias cudaSurfaceFormatMode = int;
enum : cudaSurfaceFormatMode {
  cudaFormatModeForced = 0,
  cudaFormatModeAuto = 1
}

/**
 * CUDA Surface reference
 */
struct surfaceReference {
  cudaChannelFormatDesc channelDesc;
}

/**
 * An opaque value that represents a CUDA Surface object
 */
alias cudaSurfaceObject_t = ulong;


// texture_types.h

enum cudaTextureType1D              = 0x01;
enum cudaTextureType2D              = 0x02;
enum cudaTextureType3D              = 0x03;
enum cudaTextureTypeCubemap         = 0x0C;
enum cudaTextureType1DLayered       = 0xF1;
enum cudaTextureType2DLayered       = 0xF2;
enum cudaTextureTypeCubemapLayered  = 0xFC;

/**
 * CUDA texture address modes
 */
alias cudaTextureAddressMode = int;
enum : cudaTextureAddressMode {
  cudaAddressModeWrap   = 0,
  cudaAddressModeClamp  = 1,
  cudaAddressModeMirror = 2,
  cudaAddressModeBorder = 3
}

/**
 * CUDA texture filter modes
 */
alias cudaTextureFilterMode = int;
enum : cudaTextureFilterMode {
  cudaFilterModePoint  = 0,
  cudaFilterModeLinear = 1
}

/**
 * CUDA texture read modes
 */
alias cudaTextureReadMode = int;
enum : cudaTextureReadMode {
  cudaReadModeElementType     = 0,
  cudaReadModeNormalizedFloat = 1
}

/**
 * CUDA texture reference
 */
struct textureReference {
  int                          normalized;
  cudaTextureFilterMode   filterMode;
  cudaTextureAddressMode[3]  addressMode;
  cudaChannelFormatDesc channelDesc;
  int                          sRGB;
  uint                 maxAnisotropy;
  cudaTextureFilterMode   mipmapFilterMode;
  float                        mipmapLevelBias;
  float                        minMipmapLevelClamp;
  float                        maxMipmapLevelClamp;
  int[15]                          __cudaReserved;
}

/**
 * CUDA texture descriptor
 */
struct cudaTextureDesc {
  cudaTextureAddressMode[3] addressMode;
  cudaTextureFilterMode  filterMode;
  cudaTextureReadMode    readMode;
  int                         sRGB;
  float[4]                       borderColor;
  int                         normalizedCoords;
  uint                maxAnisotropy;
  cudaTextureFilterMode  mipmapFilterMode;
  float                       mipmapLevelBias;
  float                       minMipmapLevelClamp;
  float                       maxMipmapLevelClamp;
}

/**
 * An opaque value that represents a CUDA texture object
 */
alias cudaTextureObject_t = ulong;

alias cuFloatComplex = float2;
alias cuDoubleComplex = double2;
alias cuComplex = cuFloatComplex;


// cuda_runtime_api.h

extern(System) nothrow {
  alias cudaStreamCallback_t = void function(cudaStream_t stream, cudaError_t status, void *userData);
}


extern(System) @nogc nothrow {
  alias da_cudaDeviceReset = cudaError_t function();
  alias da_cudaDeviceSynchronize = cudaError_t function();
  alias da_cudaDeviceSetLimit = cudaError_t function(cudaLimit limit, size_t value);
  alias da_cudaDeviceGetLimit = cudaError_t function(size_t *pValue, cudaLimit limit);
  alias da_cudaDeviceGetCacheConfig = cudaError_t function(cudaFuncCache *pCacheConfig);
  alias da_cudaDeviceGetStreamPriorityRange = cudaError_t function(int *leastPriority, int *greatestPriority);
  alias da_cudaDeviceSetCacheConfig = cudaError_t function(cudaFuncCache cacheConfig);
  alias da_cudaDeviceGetSharedMemConfig = cudaError_t function(cudaSharedMemConfig *pConfig);
  alias da_cudaDeviceSetSharedMemConfig = cudaError_t function(cudaSharedMemConfig config);
  alias da_cudaDeviceGetByPCIBusId = cudaError_t function(int *device, const char *pciBusId);
  alias da_cudaDeviceGetPCIBusId = cudaError_t function(char *pciBusId, int len, int device);
  alias da_cudaIpcGetEventHandle = cudaError_t function(cudaIpcEventHandle_t *handle, cudaEvent_t event);
  alias da_cudaIpcOpenEventHandle = cudaError_t function(cudaEvent_t *event, cudaIpcEventHandle_t handle);
  alias da_cudaIpcGetMemHandle = cudaError_t function(cudaIpcMemHandle_t *handle, void *devPtr);
  alias da_cudaIpcOpenMemHandle = cudaError_t function(void **devPtr, cudaIpcMemHandle_t handle, uint flags);
  alias da_cudaIpcCloseMemHandle = cudaError_t function(void *devPtr);
  alias da_cudaThreadExit = cudaError_t function();
  alias da_cudaThreadSynchronize = cudaError_t function();
  alias da_cudaThreadSetLimit = cudaError_t function(cudaLimit limit, size_t value);
  alias da_cudaThreadGetLimit = cudaError_t function(size_t *pValue, cudaLimit limit);
  alias da_cudaThreadGetCacheConfig = cudaError_t function(cudaFuncCache *pCacheConfig);
  alias da_cudaThreadSetCacheConfig = cudaError_t function(cudaFuncCache cacheConfig);
  alias da_cudaGetLastError = cudaError_t function();
  alias da_cudaPeekAtLastError = cudaError_t function();
  alias da_cudaGetErrorName = const char* function(cudaError_t error);
  alias da_cudaGetErrorString = const char* function(cudaError_t error);
  alias da_cudaGetDeviceCount = cudaError_t function(int *count);
  alias da_cudaGetDeviceProperties = cudaError_t function(cudaDeviceProp *prop, int device);
  alias da_cudaDeviceGetAttribute = cudaError_t function(int *value, cudaDeviceAttr attr, int device);
  alias da_cudaDeviceGetP2PAttribute = cudaError_t function(int *value, cudaDeviceP2PAttr attr, int srcDevice, int dstDevice);
  alias da_cudaChooseDevice = cudaError_t function(int *device, const cudaDeviceProp *prop);
  alias da_cudaSetDevice = cudaError_t function(int device);
  alias da_cudaGetDevice = cudaError_t function(int *device);
  alias da_cudaSetValidDevices = cudaError_t function(int *device_arr, int len);
  alias da_cudaSetDeviceFlags = cudaError_t function( uint flags );
  alias da_cudaGetDeviceFlags = cudaError_t function( uint *flags );
  alias da_cudaStreamCreate = cudaError_t function(cudaStream_t *pStream);
  alias da_cudaStreamCreateWithFlags = cudaError_t function(cudaStream_t *pStream, uint flags);
  alias da_cudaStreamCreateWithPriority = cudaError_t function(cudaStream_t *pStream, uint flags, int priority);
  alias da_cudaStreamGetPriority = cudaError_t function(cudaStream_t hStream, int *priority);
  alias da_cudaStreamGetFlags = cudaError_t function(cudaStream_t hStream, uint *flags);
  alias da_cudaStreamDestroy = cudaError_t function(cudaStream_t stream);
  alias da_cudaStreamWaitEvent = cudaError_t function(cudaStream_t stream, cudaEvent_t event, uint flags);
  alias da_cudaStreamAddCallback = cudaError_t function(cudaStream_t stream, cudaStreamCallback_t callback, void *userData, uint flags);
  alias da_cudaStreamSynchronize = cudaError_t function(cudaStream_t stream);
  alias da_cudaStreamQuery = cudaError_t function(cudaStream_t stream);
  alias da_cudaStreamAttachMemAsync = cudaError_t function(cudaStream_t stream, void *devPtr, size_t length = 0, uint flags = cudaMemAttachSingle);
  alias da_cudaStreamBeginCapture = cudaError_t function(cudaStream_t stream);
  alias da_cudaStreamEndCapture = cudaError_t function(cudaStream_t stream, cudaGraph_t *pGraph);
  alias da_cudaStreamIsCapturing = cudaError_t function(cudaStream_t stream, cudaStreamCaptureStatus *pCaptureStatus);
  alias da_cudaEventCreate = cudaError_t function(cudaEvent_t *event);
  alias da_cudaEventCreateWithFlags = cudaError_t function(cudaEvent_t *event, uint flags);
  alias da_cudaEventRecord = cudaError_t function(cudaEvent_t event, cudaStream_t stream = null);
  alias da_cudaEventQuery = cudaError_t function(cudaEvent_t event);
  alias da_cudaEventSynchronize = cudaError_t function(cudaEvent_t event);
  alias da_cudaEventDestroy = cudaError_t function(cudaEvent_t event);
  alias da_cudaEventElapsedTime = cudaError_t function(float *ms, cudaEvent_t start, cudaEvent_t end);
  alias da_cudaImportExternalMemory = cudaError_t function(cudaExternalMemory_t *extMem_out, const cudaExternalMemoryHandleDesc *memHandleDesc);
  alias da_cudaExternalMemoryGetMappedBuffer = cudaError_t function(void **devPtr, cudaExternalMemory_t extMem, const cudaExternalMemoryBufferDesc *bufferDesc);
  alias da_cudaExternalMemoryGetMappedMipmappedArray = cudaError_t function(cudaMipmappedArray_t *mipmap, cudaExternalMemory_t extMem, const cudaExternalMemoryMipmappedArrayDesc *mipmapDesc);
  alias da_cudaDestroyExternalMemory = cudaError_t function(cudaExternalMemory_t extMem);
  alias da_cudaImportExternalSemaphore = cudaError_t function(cudaExternalSemaphore_t *extSem_out, const cudaExternalSemaphoreHandleDesc *semHandleDesc);
  alias da_cudaSignalExternalSemaphoresAsync = cudaError_t function(const cudaExternalSemaphore_t *extSemArray, const cudaExternalSemaphoreSignalParams *paramsArray, uint numExtSems, cudaStream_t stream = null);
  alias da_cudaWaitExternalSemaphoresAsync = cudaError_t function(const cudaExternalSemaphore_t *extSemArray, const cudaExternalSemaphoreWaitParams *paramsArray, uint numExtSems, cudaStream_t stream = null);
  alias da_cudaDestroyExternalSemaphore = cudaError_t function(cudaExternalSemaphore_t extSem);
  alias da_cudaLaunchKernel = cudaError_t function(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream);
  alias da_cudaLaunchCooperativeKernel = cudaError_t function(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream);
  alias da_cudaLaunchCooperativeKernelMultiDevice = cudaError_t function(cudaLaunchParams *launchParamsList, uint numDevices, uint flags  = 0);
  alias da_cudaFuncSetCacheConfig = cudaError_t function(const void *func, cudaFuncCache cacheConfig);
  alias da_cudaFuncSetSharedMemConfig = cudaError_t function(const void *func, cudaSharedMemConfig config);
  alias da_cudaFuncGetAttributes = cudaError_t function(cudaFuncAttributes *attr, const void *func);
  alias da_cudaFuncSetAttribute = cudaError_t function(const void *func, cudaFuncAttribute attr, int value);
  alias da_cudaSetDoubleForDevice = cudaError_t function(double *d);
  alias da_cudaSetDoubleForHost = cudaError_t function(double *d);
  alias da_cudaLaunchHostFunc = cudaError_t function(cudaStream_t stream, cudaHostFn_t fn, void *userData);
  alias da_cudaOccupancyMaxActiveBlocksPerMultiprocessor = cudaError_t function(int *numBlocks, const void *func, int blockSize, size_t dynamicSMemSize);
  alias da_cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags = cudaError_t function(int *numBlocks, const void *func, int blockSize, size_t dynamicSMemSize, uint flags);
  alias da_cudaConfigureCall = cudaError_t function(dim3 gridDim, dim3 blockDim, size_t sharedMem = 0, cudaStream_t stream = null);
  alias da_cudaSetupArgument = cudaError_t function(const void *arg, size_t size, size_t offset);
  alias da_cudaLaunch = cudaError_t function(const void *func);
  alias da_cudaMallocManaged = cudaError_t function(void **devPtr, size_t size, uint flags = cudaMemAttachGlobal);
  alias da_cudaMalloc = cudaError_t function(void **devPtr, size_t size);
  alias da_cudaMallocHost = cudaError_t function(void **ptr, size_t size);
  alias da_cudaMallocPitch = cudaError_t function(void **devPtr, size_t *pitch, size_t width, size_t height);
  alias da_cudaMallocArray = cudaError_t function(cudaArray_t *array, const cudaChannelFormatDesc *desc, size_t width, size_t height = 0, uint flags = 0);
  alias da_cudaFree = cudaError_t function(void *devPtr);
  alias da_cudaFreeHost = cudaError_t function(void *ptr);
  alias da_cudaFreeArray = cudaError_t function(cudaArray_t array);
  alias da_cudaFreeMipmappedArray = cudaError_t function(cudaMipmappedArray_t mipmappedArray);
  alias da_cudaHostAlloc = cudaError_t function(void **pHost, size_t size, uint flags);
  alias da_cudaHostRegister = cudaError_t function(void *ptr, size_t size, uint flags);
  alias da_cudaHostUnregister = cudaError_t function(void *ptr);
  alias da_cudaHostGetDevicePointer = cudaError_t function(void **pDevice, void *pHost, uint flags);
  alias da_cudaHostGetFlags = cudaError_t function(uint *pFlags, void *pHost);
  alias da_cudaMalloc3D = cudaError_t function(cudaPitchedPtr* pitchedDevPtr, cudaExtent extent);
  alias da_cudaMalloc3DArray = cudaError_t function(cudaArray_t *array, const cudaChannelFormatDesc* desc, cudaExtent extent, uint flags = 0);
  alias da_cudaMallocMipmappedArray = cudaError_t function(cudaMipmappedArray_t *mipmappedArray, const cudaChannelFormatDesc* desc, cudaExtent extent, uint numLevels, uint flags = 0);
  alias da_cudaGetMipmappedArrayLevel = cudaError_t function(cudaArray_t *levelArray, cudaMipmappedArray_const_t mipmappedArray, uint level);
  alias da_cudaMemcpy3D = cudaError_t function(const cudaMemcpy3DParms *p);
  alias da_cudaMemcpy3DPeer = cudaError_t function(const cudaMemcpy3DPeerParms *p);
  alias da_cudaMemcpy3DAsync = cudaError_t function(const cudaMemcpy3DParms *p, cudaStream_t stream = null);
  alias da_cudaMemcpy3DPeerAsync = cudaError_t function(const cudaMemcpy3DPeerParms *p, cudaStream_t stream = null);
  alias da_cudaMemGetInfo = cudaError_t function(size_t *free, size_t *total);
  alias da_cudaArrayGetInfo = cudaError_t function(cudaChannelFormatDesc *desc, cudaExtent *extent, uint *flags, cudaArray_t array);
  alias da_cudaMemcpy = cudaError_t function(void *dst, const void *src, size_t count, cudaMemcpyKind kind);
  alias da_cudaMemcpyPeer = cudaError_t function(void *dst, int dstDevice, const void *src, int srcDevice, size_t count);
  alias da_cudaMemcpyToArray = cudaError_t function(cudaArray_t dst, size_t wOffset, size_t hOffset, const void *src, size_t count, cudaMemcpyKind kind);
  alias da_cudaMemcpyFromArray = cudaError_t function(void *dst, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t count, cudaMemcpyKind kind);
  alias da_cudaMemcpyArrayToArray = cudaError_t function(cudaArray_t dst, size_t wOffsetDst, size_t hOffsetDst, cudaArray_const_t src, size_t wOffsetSrc, size_t hOffsetSrc, size_t count, cudaMemcpyKind kind = cudaMemcpyDeviceToDevice);
  alias da_cudaMemcpy2D = cudaError_t function(void *dst, size_t dpitch, const void *src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind);
  alias da_cudaMemcpy2DToArray = cudaError_t function(cudaArray_t dst, size_t wOffset, size_t hOffset, const void *src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind);
  alias da_cudaMemcpy2DFromArray = cudaError_t function(void *dst, size_t dpitch, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t width, size_t height, cudaMemcpyKind kind);
  alias da_cudaMemcpy2DArrayToArray = cudaError_t function(cudaArray_t dst, size_t wOffsetDst, size_t hOffsetDst, cudaArray_const_t src, size_t wOffsetSrc, size_t hOffsetSrc, size_t width, size_t height, cudaMemcpyKind kind = cudaMemcpyDeviceToDevice);
  alias da_cudaMemcpyToSymbol = cudaError_t function(const void *symbol, const void *src, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyHostToDevice);
  alias da_cudaMemcpyFromSymbol = cudaError_t function(void *dst, const void *symbol, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyDeviceToHost);
  alias da_cudaMemcpyAsync = cudaError_t function(void *dst, const void *src, size_t count, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpyPeerAsync = cudaError_t function(void *dst, int dstDevice, const void *src, int srcDevice, size_t count, cudaStream_t stream = null);
  alias da_cudaMemcpyToArrayAsync = cudaError_t function(cudaArray_t dst, size_t wOffset, size_t hOffset, const void *src, size_t count, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpyFromArrayAsync = cudaError_t function(void *dst, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t count, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpy2DAsync = cudaError_t function(void *dst, size_t dpitch, const void *src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpy2DToArrayAsync = cudaError_t function(cudaArray_t dst, size_t wOffset, size_t hOffset, const void *src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpy2DFromArrayAsync = cudaError_t function(void *dst, size_t dpitch, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpyToSymbolAsync = cudaError_t function(const void *symbol, const void *src, size_t count, size_t offset, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemcpyFromSymbolAsync = cudaError_t function(void *dst, const void *symbol, size_t count, size_t offset, cudaMemcpyKind kind, cudaStream_t stream = null);
  alias da_cudaMemset = cudaError_t function(void *devPtr, int value, size_t count);
  alias da_cudaMemset2D = cudaError_t function(void *devPtr, size_t pitch, int value, size_t width, size_t height);
  alias da_cudaMemset3D = cudaError_t function(cudaPitchedPtr pitchedDevPtr, int value, cudaExtent extent);
  alias da_cudaMemsetAsync = cudaError_t function(void *devPtr, int value, size_t count, cudaStream_t stream = null);
  alias da_cudaMemset2DAsync = cudaError_t function(void *devPtr, size_t pitch, int value, size_t width, size_t height, cudaStream_t stream = null);
  alias da_cudaMemset3DAsync = cudaError_t function(cudaPitchedPtr pitchedDevPtr, int value, cudaExtent extent, cudaStream_t stream = null);
  alias da_cudaGetSymbolAddress = cudaError_t function(void **devPtr, const void *symbol);
  alias da_cudaGetSymbolSize = cudaError_t function(size_t *size, const void *symbol);
  alias da_cudaMemPrefetchAsync = cudaError_t function(const void *devPtr, size_t count, int dstDevice, cudaStream_t stream = null);
  alias da_cudaMemAdvise = cudaError_t function(const void *devPtr, size_t count, cudaMemoryAdvise advice, int device);
  alias da_cudaMemRangeGetAttribute = cudaError_t function(void *data, size_t dataSize, cudaMemRangeAttribute attribute, const void *devPtr, size_t count);
  alias da_cudaMemRangeGetAttributes = cudaError_t function(void **data, size_t *dataSizes, cudaMemRangeAttribute *attributes, size_t numAttributes, const void *devPtr, size_t count);
  alias da_cudaPointerGetAttributes = cudaError_t function(cudaPointerAttributes *attributes, const void *ptr);
  alias da_cudaDeviceCanAccessPeer = cudaError_t function(int *canAccessPeer, int device, int peerDevice);
  alias da_cudaDeviceEnablePeerAccess = cudaError_t function(int peerDevice, uint flags);
  alias da_cudaDeviceDisablePeerAccess = cudaError_t function(int peerDevice);
  alias da_cudaGraphicsUnregisterResource = cudaError_t function(cudaGraphicsResource_t resource);
  alias da_cudaGraphicsResourceSetMapFlags = cudaError_t function(cudaGraphicsResource_t resource, uint flags);
  alias da_cudaGraphicsMapResources = cudaError_t function(int count, cudaGraphicsResource_t *resources, cudaStream_t stream = null);
  alias da_cudaGraphicsUnmapResources = cudaError_t function(int count, cudaGraphicsResource_t *resources, cudaStream_t stream = null);
  alias da_cudaGraphicsResourceGetMappedPointer = cudaError_t function(void **devPtr, size_t *size, cudaGraphicsResource_t resource);
  alias da_cudaGraphicsSubResourceGetMappedArray = cudaError_t function(cudaArray_t *array, cudaGraphicsResource_t resource, uint arrayIndex, uint mipLevel);
  alias da_cudaGraphicsResourceGetMappedMipmappedArray = cudaError_t function(cudaMipmappedArray_t *mipmappedArray, cudaGraphicsResource_t resource);
  alias da_cudaGetChannelDesc = cudaError_t function(cudaChannelFormatDesc *desc, cudaArray_const_t array);
  alias da_cudaBindTexture = cudaError_t function(size_t *offset, const textureReference *texref, const void *devPtr, const cudaChannelFormatDesc *desc, size_t size = uint.max);
  alias da_cudaBindTexture2D = cudaError_t function(size_t *offset, const textureReference *texref, const void *devPtr, const cudaChannelFormatDesc *desc, size_t width, size_t height, size_t pitch);
  alias da_cudaBindTextureToArray = cudaError_t function(const textureReference *texref, cudaArray_const_t array, const cudaChannelFormatDesc *desc);
  alias da_cudaBindTextureToMipmappedArray = cudaError_t function(const textureReference *texref, cudaMipmappedArray_const_t mipmappedArray, const cudaChannelFormatDesc *desc);
  alias da_cudaUnbindTexture = cudaError_t function(const textureReference *texref);
  alias da_cudaGetTextureAlignmentOffset = cudaError_t function(size_t *offset, const textureReference *texref);
  alias da_cudaGetTextureReference = cudaError_t function(const textureReference **texref, const void *symbol);
  alias da_cudaBindSurfaceToArray = cudaError_t function(const surfaceReference *surfref, cudaArray_const_t array, const cudaChannelFormatDesc *desc);
  alias da_cudaGetSurfaceReference = cudaError_t function(const surfaceReference **surfref, const void *symbol);
  alias da_cudaCreateTextureObject = cudaError_t function(cudaTextureObject_t *pTexObject, const cudaResourceDesc *pResDesc, const cudaTextureDesc *pTexDesc, const cudaResourceViewDesc *pResViewDesc);
  alias da_cudaDestroyTextureObject = cudaError_t function(cudaTextureObject_t texObject);
  alias da_cudaGetTextureObjectResourceDesc = cudaError_t function(cudaResourceDesc *pResDesc, cudaTextureObject_t texObject);
  alias da_cudaGetTextureObjectTextureDesc = cudaError_t function(cudaTextureDesc *pTexDesc, cudaTextureObject_t texObject);
  alias da_cudaGetTextureObjectResourceViewDesc = cudaError_t function(cudaResourceViewDesc *pResViewDesc, cudaTextureObject_t texObject);
  alias da_cudaCreateSurfaceObject = cudaError_t function(cudaSurfaceObject_t *pSurfObject, const cudaResourceDesc *pResDesc);
  alias da_cudaDestroySurfaceObject = cudaError_t function(cudaSurfaceObject_t surfObject);
  alias da_cudaGetSurfaceObjectResourceDesc = cudaError_t function(cudaResourceDesc *pResDesc, cudaSurfaceObject_t surfObject);
  alias da_cudaDriverGetVersion = cudaError_t function(int *driverVersion);
  alias da_cudaRuntimeGetVersion = cudaError_t function(int *runtimeVersion);
  alias da_cudaGraphCreate = cudaError_t function(cudaGraph_t *pGraph, uint flags);
  alias da_cudaGraphAddKernelNode = cudaError_t function(cudaGraphNode_t *pGraphNode, cudaGraph_t graph, cudaGraphNode_t *pDependencies, size_t numDependencies, const cudaKernelNodeParams *pNodeParams);
  alias da_cudaGraphKernelNodeGetParams = cudaError_t function(cudaGraphNode_t node, cudaKernelNodeParams *pNodeParams);
  alias da_cudaGraphKernelNodeSetParams = cudaError_t function(cudaGraphNode_t node, const cudaKernelNodeParams *pNodeParams);
  alias da_cudaGraphAddMemcpyNode = cudaError_t function(cudaGraphNode_t *pGraphNode, cudaGraph_t graph, cudaGraphNode_t *pDependencies, size_t numDependencies, const cudaMemcpy3DParms *pCopyParams);
  alias da_cudaGraphMemcpyNodeGetParams = cudaError_t function(cudaGraphNode_t node, cudaMemcpy3DParms *pNodeParams);
  alias da_cudaGraphMemcpyNodeSetParams = cudaError_t function(cudaGraphNode_t node, const cudaMemcpy3DParms *pNodeParams);
  alias da_cudaGraphAddMemsetNode = cudaError_t function(cudaGraphNode_t *pGraphNode, cudaGraph_t graph, cudaGraphNode_t *pDependencies, size_t numDependencies, const cudaMemsetParams *pMemsetParams);
  alias da_cudaGraphMemsetNodeGetParams = cudaError_t function(cudaGraphNode_t node, cudaMemsetParams *pNodeParams);
  alias da_cudaGraphMemsetNodeSetParams = cudaError_t function(cudaGraphNode_t node, const cudaMemsetParams *pNodeParams);
  alias da_cudaGraphAddHostNode = cudaError_t function(cudaGraphNode_t *pGraphNode, cudaGraph_t graph, cudaGraphNode_t *pDependencies, size_t numDependencies, const cudaHostNodeParams *pNodeParams);
  alias da_cudaGraphHostNodeGetParams = cudaError_t function(cudaGraphNode_t node, cudaHostNodeParams *pNodeParams);
  alias da_cudaGraphHostNodeSetParams = cudaError_t function(cudaGraphNode_t node, const cudaHostNodeParams *pNodeParams);
  alias da_cudaGraphAddChildGraphNode = cudaError_t function(cudaGraphNode_t *pGraphNode, cudaGraph_t graph, cudaGraphNode_t *pDependencies, size_t numDependencies, cudaGraph_t childGraph);
  alias da_cudaGraphChildGraphNodeGetGraph = cudaError_t function(cudaGraphNode_t node, cudaGraph_t *pGraph);
  alias da_cudaGraphAddEmptyNode = cudaError_t function(cudaGraphNode_t *pGraphNode, cudaGraph_t graph, cudaGraphNode_t *pDependencies, size_t numDependencies);
  alias da_cudaGraphClone = cudaError_t function(cudaGraph_t *pGraphClone, cudaGraph_t originalGraph);
  alias da_cudaGraphNodeFindInClone = cudaError_t function(cudaGraphNode_t *pNode, cudaGraphNode_t originalNode, cudaGraph_t clonedGraph);
  alias da_cudaGraphNodeGetType = cudaError_t function(cudaGraphNode_t node, cudaGraphNodeType *pType);
  alias da_cudaGraphGetNodes = cudaError_t function(cudaGraph_t graph, cudaGraphNode_t *nodes, size_t *numNodes);
  alias da_cudaGraphGetRootNodes = cudaError_t function(cudaGraph_t graph, cudaGraphNode_t *pRootNodes, size_t *pNumRootNodes);
  alias da_cudaGraphGetEdges = cudaError_t function(cudaGraph_t graph, cudaGraphNode_t *from, cudaGraphNode_t *to, size_t *numEdges);
  alias da_cudaGraphNodeGetDependencies = cudaError_t function(cudaGraphNode_t node, cudaGraphNode_t *pDependencies, size_t *pNumDependencies);
  alias da_cudaGraphNodeGetDependentNodes = cudaError_t function(cudaGraphNode_t node, cudaGraphNode_t *pDependentNodes, size_t *pNumDependentNodes);
  alias da_cudaGraphAddDependencies = cudaError_t function(cudaGraph_t graph, cudaGraphNode_t *from, cudaGraphNode_t *to, size_t numDependencies);
  alias da_cudaGraphRemoveDependencies = cudaError_t function(cudaGraph_t graph, cudaGraphNode_t *from, cudaGraphNode_t *to, size_t numDependencies);
  alias da_cudaGraphDestroyNode = cudaError_t function(cudaGraphNode_t node);
  alias da_cudaGraphInstantiate = cudaError_t function(cudaGraphExec_t *pGraphExec, cudaGraph_t graph, cudaGraphNode_t *pErrorNode, char *pLogBuffer, size_t bufferSize);
  alias da_cudaGraphLaunch = cudaError_t function(cudaGraphExec_t graphExec, cudaStream_t stream);
  alias da_cudaGraphExecDestroy = cudaError_t function(cudaGraphExec_t graphExec);
  alias da_cudaGraphDestroy = cudaError_t function(cudaGraph_t graph);
  alias da_cudaGetExportTable = cudaError_t function(const void **ppExportTable, const cudaUUID_t *pExportTableId);

}

__gshared
{
  da_cudaDeviceReset cudaDeviceReset;
  da_cudaDeviceSynchronize cudaDeviceSynchronize;
  da_cudaDeviceSetLimit cudaDeviceSetLimit;
  da_cudaDeviceGetLimit cudaDeviceGetLimit;
  da_cudaDeviceGetCacheConfig cudaDeviceGetCacheConfig;
  da_cudaDeviceGetStreamPriorityRange cudaDeviceGetStreamPriorityRange;
  da_cudaDeviceSetCacheConfig cudaDeviceSetCacheConfig;
  da_cudaDeviceGetSharedMemConfig cudaDeviceGetSharedMemConfig;
  da_cudaDeviceSetSharedMemConfig cudaDeviceSetSharedMemConfig;
  da_cudaDeviceGetByPCIBusId cudaDeviceGetByPCIBusId;
  da_cudaDeviceGetPCIBusId cudaDeviceGetPCIBusId;
  da_cudaIpcGetEventHandle cudaIpcGetEventHandle;
  da_cudaIpcOpenEventHandle cudaIpcOpenEventHandle;
  da_cudaIpcGetMemHandle cudaIpcGetMemHandle;
  da_cudaIpcOpenMemHandle cudaIpcOpenMemHandle;
  da_cudaIpcCloseMemHandle cudaIpcCloseMemHandle;
  da_cudaThreadExit cudaThreadExit;
  da_cudaThreadSynchronize cudaThreadSynchronize;
  da_cudaThreadSetLimit cudaThreadSetLimit;
  da_cudaThreadGetLimit cudaThreadGetLimit;
  da_cudaThreadGetCacheConfig cudaThreadGetCacheConfig;
  da_cudaThreadSetCacheConfig cudaThreadSetCacheConfig;
  da_cudaGetLastError cudaGetLastError;
  da_cudaPeekAtLastError cudaPeekAtLastError;
  da_cudaGetErrorName cudaGetErrorName;
  da_cudaGetErrorString cudaGetErrorString;
  da_cudaGetDeviceCount cudaGetDeviceCount;
  da_cudaGetDeviceProperties cudaGetDeviceProperties;
  da_cudaDeviceGetAttribute cudaDeviceGetAttribute;
  da_cudaDeviceGetP2PAttribute cudaDeviceGetP2PAttribute;
  da_cudaChooseDevice cudaChooseDevice;
  da_cudaSetDevice cudaSetDevice;
  da_cudaGetDevice cudaGetDevice;
  da_cudaSetValidDevices cudaSetValidDevices;
  da_cudaSetDeviceFlags cudaSetDeviceFlags;
  da_cudaGetDeviceFlags cudaGetDeviceFlags;
  da_cudaStreamCreate cudaStreamCreate;
  da_cudaStreamCreateWithFlags cudaStreamCreateWithFlags;
  da_cudaStreamCreateWithPriority cudaStreamCreateWithPriority;
  da_cudaStreamGetPriority cudaStreamGetPriority;
  da_cudaStreamGetFlags cudaStreamGetFlags;
  da_cudaStreamDestroy cudaStreamDestroy;
  da_cudaStreamWaitEvent cudaStreamWaitEvent;
  da_cudaStreamAddCallback cudaStreamAddCallback;
  da_cudaStreamSynchronize cudaStreamSynchronize;
  da_cudaStreamQuery cudaStreamQuery;
  da_cudaStreamAttachMemAsync cudaStreamAttachMemAsync;
  da_cudaStreamBeginCapture cudaStreamBeginCapture;
  da_cudaStreamEndCapture cudaStreamEndCapture;
  da_cudaStreamIsCapturing cudaStreamIsCapturing;
  da_cudaEventCreate cudaEventCreate;
  da_cudaEventCreateWithFlags cudaEventCreateWithFlags;
  da_cudaEventRecord cudaEventRecord;
  da_cudaEventQuery cudaEventQuery;
  da_cudaEventSynchronize cudaEventSynchronize;
  da_cudaEventDestroy cudaEventDestroy;
  da_cudaEventElapsedTime cudaEventElapsedTime;
  da_cudaImportExternalMemory cudaImportExternalMemory;
  da_cudaExternalMemoryGetMappedBuffer cudaExternalMemoryGetMappedBuffer;
  da_cudaExternalMemoryGetMappedMipmappedArray cudaExternalMemoryGetMappedMipmappedArray;
  da_cudaDestroyExternalMemory cudaDestroyExternalMemory;
  da_cudaImportExternalSemaphore cudaImportExternalSemaphore;
  da_cudaSignalExternalSemaphoresAsync cudaSignalExternalSemaphoresAsync;
  da_cudaWaitExternalSemaphoresAsync cudaWaitExternalSemaphoresAsync;
  da_cudaDestroyExternalSemaphore cudaDestroyExternalSemaphore;
  da_cudaLaunchKernel cudaLaunchKernel;
  da_cudaLaunchCooperativeKernel cudaLaunchCooperativeKernel;
  da_cudaLaunchCooperativeKernelMultiDevice cudaLaunchCooperativeKernelMultiDevice;
  da_cudaFuncSetCacheConfig cudaFuncSetCacheConfig;
  da_cudaFuncSetSharedMemConfig cudaFuncSetSharedMemConfig;
  da_cudaFuncGetAttributes cudaFuncGetAttributes;
  da_cudaFuncSetAttribute cudaFuncSetAttribute;
  da_cudaSetDoubleForDevice cudaSetDoubleForDevice;
  da_cudaSetDoubleForHost cudaSetDoubleForHost;
  da_cudaLaunchHostFunc cudaLaunchHostFunc;
  da_cudaOccupancyMaxActiveBlocksPerMultiprocessor cudaOccupancyMaxActiveBlocksPerMultiprocessor;
  da_cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags;
  da_cudaConfigureCall cudaConfigureCall;
  da_cudaSetupArgument cudaSetupArgument;
  da_cudaLaunch cudaLaunch;
  da_cudaMallocManaged cudaMallocManaged;
  da_cudaMalloc cudaMalloc;
  da_cudaMallocHost cudaMallocHost;
  da_cudaMallocPitch cudaMallocPitch;
  da_cudaMallocArray cudaMallocArray;
  da_cudaFree cudaFree;
  da_cudaFreeHost cudaFreeHost;
  da_cudaFreeArray cudaFreeArray;
  da_cudaFreeMipmappedArray cudaFreeMipmappedArray;
  da_cudaHostAlloc cudaHostAlloc;
  da_cudaHostRegister cudaHostRegister;
  da_cudaHostUnregister cudaHostUnregister;
  da_cudaHostGetDevicePointer cudaHostGetDevicePointer;
  da_cudaHostGetFlags cudaHostGetFlags;
  da_cudaMalloc3D cudaMalloc3D;
  da_cudaMalloc3DArray cudaMalloc3DArray;
  da_cudaMallocMipmappedArray cudaMallocMipmappedArray;
  da_cudaGetMipmappedArrayLevel cudaGetMipmappedArrayLevel;
  da_cudaMemcpy3D cudaMemcpy3D;
  da_cudaMemcpy3DPeer cudaMemcpy3DPeer;
  da_cudaMemcpy3DAsync cudaMemcpy3DAsync;
  da_cudaMemcpy3DPeerAsync cudaMemcpy3DPeerAsync;
  da_cudaMemGetInfo cudaMemGetInfo;
  da_cudaArrayGetInfo cudaArrayGetInfo;
  da_cudaMemcpy cudaMemcpy;
  da_cudaMemcpyPeer cudaMemcpyPeer;
  da_cudaMemcpyToArray cudaMemcpyToArray;
  da_cudaMemcpyFromArray cudaMemcpyFromArray;
  da_cudaMemcpyArrayToArray cudaMemcpyArrayToArray;
  da_cudaMemcpy2D cudaMemcpy2D;
  da_cudaMemcpy2DToArray cudaMemcpy2DToArray;
  da_cudaMemcpy2DFromArray cudaMemcpy2DFromArray;
  da_cudaMemcpy2DArrayToArray cudaMemcpy2DArrayToArray;
  da_cudaMemcpyToSymbol cudaMemcpyToSymbol;
  da_cudaMemcpyFromSymbol cudaMemcpyFromSymbol;
  da_cudaMemcpyAsync cudaMemcpyAsync;
  da_cudaMemcpyPeerAsync cudaMemcpyPeerAsync;
  da_cudaMemcpyToArrayAsync cudaMemcpyToArrayAsync;
  da_cudaMemcpyFromArrayAsync cudaMemcpyFromArrayAsync;
  da_cudaMemcpy2DAsync cudaMemcpy2DAsync;
  da_cudaMemcpy2DToArrayAsync cudaMemcpy2DToArrayAsync;
  da_cudaMemcpy2DFromArrayAsync cudaMemcpy2DFromArrayAsync;
  da_cudaMemcpyToSymbolAsync cudaMemcpyToSymbolAsync;
  da_cudaMemcpyFromSymbolAsync cudaMemcpyFromSymbolAsync;
  da_cudaMemset cudaMemset;
  da_cudaMemset2D cudaMemset2D;
  da_cudaMemset3D cudaMemset3D;
  da_cudaMemsetAsync cudaMemsetAsync;
  da_cudaMemset2DAsync cudaMemset2DAsync;
  da_cudaMemset3DAsync cudaMemset3DAsync;
  da_cudaGetSymbolAddress cudaGetSymbolAddress;
  da_cudaGetSymbolSize cudaGetSymbolSize;
  da_cudaMemPrefetchAsync cudaMemPrefetchAsync;
  da_cudaMemAdvise cudaMemAdvise;
  da_cudaMemRangeGetAttribute cudaMemRangeGetAttribute;
  da_cudaMemRangeGetAttributes cudaMemRangeGetAttributes;
  da_cudaPointerGetAttributes cudaPointerGetAttributes;
  da_cudaDeviceCanAccessPeer cudaDeviceCanAccessPeer;
  da_cudaDeviceEnablePeerAccess cudaDeviceEnablePeerAccess;
  da_cudaDeviceDisablePeerAccess cudaDeviceDisablePeerAccess;
  da_cudaGraphicsUnregisterResource cudaGraphicsUnregisterResource;
  da_cudaGraphicsResourceSetMapFlags cudaGraphicsResourceSetMapFlags;
  da_cudaGraphicsMapResources cudaGraphicsMapResources;
  da_cudaGraphicsUnmapResources cudaGraphicsUnmapResources;
  da_cudaGraphicsResourceGetMappedPointer cudaGraphicsResourceGetMappedPointer;
  da_cudaGraphicsSubResourceGetMappedArray cudaGraphicsSubResourceGetMappedArray;
  da_cudaGraphicsResourceGetMappedMipmappedArray cudaGraphicsResourceGetMappedMipmappedArray;
  da_cudaGetChannelDesc cudaGetChannelDesc;
  da_cudaBindTexture cudaBindTexture;
  da_cudaBindTexture2D cudaBindTexture2D;
  da_cudaBindTextureToArray cudaBindTextureToArray;
  da_cudaBindTextureToMipmappedArray cudaBindTextureToMipmappedArray;
  da_cudaUnbindTexture cudaUnbindTexture;
  da_cudaGetTextureAlignmentOffset cudaGetTextureAlignmentOffset;
  da_cudaGetTextureReference cudaGetTextureReference;
  da_cudaBindSurfaceToArray cudaBindSurfaceToArray;
  da_cudaGetSurfaceReference cudaGetSurfaceReference;
  da_cudaCreateTextureObject cudaCreateTextureObject;
  da_cudaDestroyTextureObject cudaDestroyTextureObject;
  da_cudaGetTextureObjectResourceDesc cudaGetTextureObjectResourceDesc;
  da_cudaGetTextureObjectTextureDesc cudaGetTextureObjectTextureDesc;
  da_cudaGetTextureObjectResourceViewDesc cudaGetTextureObjectResourceViewDesc;
  da_cudaCreateSurfaceObject cudaCreateSurfaceObject;
  da_cudaDestroySurfaceObject cudaDestroySurfaceObject;
  da_cudaGetSurfaceObjectResourceDesc cudaGetSurfaceObjectResourceDesc;
  da_cudaDriverGetVersion cudaDriverGetVersion;
  da_cudaRuntimeGetVersion cudaRuntimeGetVersion;
  da_cudaGraphCreate cudaGraphCreate;
  da_cudaGraphAddKernelNode cudaGraphAddKernelNode;
  da_cudaGraphKernelNodeGetParams cudaGraphKernelNodeGetParams;
  da_cudaGraphKernelNodeSetParams cudaGraphKernelNodeSetParams;
  da_cudaGraphAddMemcpyNode cudaGraphAddMemcpyNode;
  da_cudaGraphMemcpyNodeGetParams cudaGraphMemcpyNodeGetParams;
  da_cudaGraphMemcpyNodeSetParams cudaGraphMemcpyNodeSetParams;
  da_cudaGraphAddMemsetNode cudaGraphAddMemsetNode;
  da_cudaGraphMemsetNodeGetParams cudaGraphMemsetNodeGetParams;
  da_cudaGraphMemsetNodeSetParams cudaGraphMemsetNodeSetParams;
  da_cudaGraphAddHostNode cudaGraphAddHostNode;
  da_cudaGraphHostNodeGetParams cudaGraphHostNodeGetParams;
  da_cudaGraphHostNodeSetParams cudaGraphHostNodeSetParams;
  da_cudaGraphAddChildGraphNode cudaGraphAddChildGraphNode;
  da_cudaGraphChildGraphNodeGetGraph cudaGraphChildGraphNodeGetGraph;
  da_cudaGraphAddEmptyNode cudaGraphAddEmptyNode;
  da_cudaGraphClone cudaGraphClone;
  da_cudaGraphNodeFindInClone cudaGraphNodeFindInClone;
  da_cudaGraphNodeGetType cudaGraphNodeGetType;
  da_cudaGraphGetNodes cudaGraphGetNodes;
  da_cudaGraphGetRootNodes cudaGraphGetRootNodes;
  da_cudaGraphGetEdges cudaGraphGetEdges;
  da_cudaGraphNodeGetDependencies cudaGraphNodeGetDependencies;
  da_cudaGraphNodeGetDependentNodes cudaGraphNodeGetDependentNodes;
  da_cudaGraphAddDependencies cudaGraphAddDependencies;
  da_cudaGraphRemoveDependencies cudaGraphRemoveDependencies;
  da_cudaGraphDestroyNode cudaGraphDestroyNode;
  da_cudaGraphInstantiate cudaGraphInstantiate;
  da_cudaGraphLaunch cudaGraphLaunch;
  da_cudaGraphExecDestroy cudaGraphExecDestroy;
  da_cudaGraphDestroy cudaGraphDestroy;
  da_cudaGetExportTable cudaGetExportTable;
}

// Runtime API loader
class DerelictCUDARuntimeLoader : SharedLibLoader
{
  protected
  {
    override void loadSymbols()
    {
      bindFunc(cast(void**)&cudaDeviceReset, "cudaDeviceReset");
      bindFunc(cast(void**)&cudaDeviceSynchronize, "cudaDeviceSynchronize");
      bindFunc(cast(void**)&cudaDeviceSetLimit, "cudaDeviceSetLimit");
      bindFunc(cast(void**)&cudaDeviceGetLimit, "cudaDeviceGetLimit");
      bindFunc(cast(void**)&cudaDeviceGetCacheConfig, "cudaDeviceGetCacheConfig");
      bindFunc(cast(void**)&cudaDeviceGetStreamPriorityRange, "cudaDeviceGetStreamPriorityRange");
      bindFunc(cast(void**)&cudaDeviceSetCacheConfig, "cudaDeviceSetCacheConfig");
      bindFunc(cast(void**)&cudaDeviceGetSharedMemConfig, "cudaDeviceGetSharedMemConfig");
      bindFunc(cast(void**)&cudaDeviceSetSharedMemConfig, "cudaDeviceSetSharedMemConfig");
      bindFunc(cast(void**)&cudaDeviceGetByPCIBusId, "cudaDeviceGetByPCIBusId");
      bindFunc(cast(void**)&cudaDeviceGetPCIBusId, "cudaDeviceGetPCIBusId");
      bindFunc(cast(void**)&cudaIpcGetEventHandle, "cudaIpcGetEventHandle");
      bindFunc(cast(void**)&cudaIpcOpenEventHandle, "cudaIpcOpenEventHandle");
      bindFunc(cast(void**)&cudaIpcGetMemHandle, "cudaIpcGetMemHandle");
      bindFunc(cast(void**)&cudaIpcOpenMemHandle, "cudaIpcOpenMemHandle");
      bindFunc(cast(void**)&cudaIpcCloseMemHandle, "cudaIpcCloseMemHandle");
      bindFunc(cast(void**)&cudaThreadExit, "cudaThreadExit");
      bindFunc(cast(void**)&cudaThreadSynchronize, "cudaThreadSynchronize");
      bindFunc(cast(void**)&cudaThreadSetLimit, "cudaThreadSetLimit");
      bindFunc(cast(void**)&cudaThreadGetLimit, "cudaThreadGetLimit");
      bindFunc(cast(void**)&cudaThreadGetCacheConfig, "cudaThreadGetCacheConfig");
      bindFunc(cast(void**)&cudaThreadSetCacheConfig, "cudaThreadSetCacheConfig");
      bindFunc(cast(void**)&cudaGetLastError, "cudaGetLastError");
      bindFunc(cast(void**)&cudaPeekAtLastError, "cudaPeekAtLastError");
      bindFunc(cast(void**)&cudaGetErrorName, "cudaGetErrorName");
      bindFunc(cast(void**)&cudaGetErrorString, "cudaGetErrorString");
      bindFunc(cast(void**)&cudaGetDeviceCount, "cudaGetDeviceCount");
      bindFunc(cast(void**)&cudaGetDeviceProperties, "cudaGetDeviceProperties");
      bindFunc(cast(void**)&cudaDeviceGetAttribute, "cudaDeviceGetAttribute");
      bindFunc(cast(void**)&cudaDeviceGetP2PAttribute, "cudaDeviceGetP2PAttribute");
      bindFunc(cast(void**)&cudaChooseDevice, "cudaChooseDevice");
      bindFunc(cast(void**)&cudaSetDevice, "cudaSetDevice");
      bindFunc(cast(void**)&cudaGetDevice, "cudaGetDevice");
      bindFunc(cast(void**)&cudaSetValidDevices, "cudaSetValidDevices");
      bindFunc(cast(void**)&cudaSetDeviceFlags, "cudaSetDeviceFlags");
      bindFunc(cast(void**)&cudaGetDeviceFlags, "cudaGetDeviceFlags");
      bindFunc(cast(void**)&cudaStreamCreate, "cudaStreamCreate");
      bindFunc(cast(void**)&cudaStreamCreateWithFlags, "cudaStreamCreateWithFlags");
      bindFunc(cast(void**)&cudaStreamCreateWithPriority, "cudaStreamCreateWithPriority");
      bindFunc(cast(void**)&cudaStreamGetPriority, "cudaStreamGetPriority");
      bindFunc(cast(void**)&cudaStreamGetFlags, "cudaStreamGetFlags");
      bindFunc(cast(void**)&cudaStreamDestroy, "cudaStreamDestroy");
      bindFunc(cast(void**)&cudaStreamWaitEvent, "cudaStreamWaitEvent");
      bindFunc(cast(void**)&cudaStreamAddCallback, "cudaStreamAddCallback");
      bindFunc(cast(void**)&cudaStreamSynchronize, "cudaStreamSynchronize");
      bindFunc(cast(void**)&cudaStreamQuery, "cudaStreamQuery");
      bindFunc(cast(void**)&cudaStreamAttachMemAsync, "cudaStreamAttachMemAsync");
      bindFunc(cast(void**)&cudaStreamBeginCapture, "cudaStreamBeginCapture");
      bindFunc(cast(void**)&cudaStreamEndCapture, "cudaStreamEndCapture");
      bindFunc(cast(void**)&cudaStreamIsCapturing, "cudaStreamIsCapturing");
      bindFunc(cast(void**)&cudaEventCreate, "cudaEventCreate");
      bindFunc(cast(void**)&cudaEventCreateWithFlags, "cudaEventCreateWithFlags");
      bindFunc(cast(void**)&cudaEventRecord, "cudaEventRecord");
      bindFunc(cast(void**)&cudaEventQuery, "cudaEventQuery");
      bindFunc(cast(void**)&cudaEventSynchronize, "cudaEventSynchronize");
      bindFunc(cast(void**)&cudaEventDestroy, "cudaEventDestroy");
      bindFunc(cast(void**)&cudaEventElapsedTime, "cudaEventElapsedTime");
      bindFunc(cast(void**)&cudaImportExternalMemory, "cudaImportExternalMemory");
      bindFunc(cast(void**)&cudaExternalMemoryGetMappedBuffer, "cudaExternalMemoryGetMappedBuffer");
      bindFunc(cast(void**)&cudaExternalMemoryGetMappedMipmappedArray, "cudaExternalMemoryGetMappedMipmappedArray");
      bindFunc(cast(void**)&cudaDestroyExternalMemory, "cudaDestroyExternalMemory");
      bindFunc(cast(void**)&cudaImportExternalSemaphore, "cudaImportExternalSemaphore");
      bindFunc(cast(void**)&cudaSignalExternalSemaphoresAsync, "cudaSignalExternalSemaphoresAsync");
      bindFunc(cast(void**)&cudaWaitExternalSemaphoresAsync, "cudaWaitExternalSemaphoresAsync");
      bindFunc(cast(void**)&cudaDestroyExternalSemaphore, "cudaDestroyExternalSemaphore");
      bindFunc(cast(void**)&cudaLaunchKernel, "cudaLaunchKernel");
      bindFunc(cast(void**)&cudaLaunchCooperativeKernel, "cudaLaunchCooperativeKernel");
      bindFunc(cast(void**)&cudaLaunchCooperativeKernelMultiDevice, "cudaLaunchCooperativeKernelMultiDevice");
      bindFunc(cast(void**)&cudaFuncSetCacheConfig, "cudaFuncSetCacheConfig");
      bindFunc(cast(void**)&cudaFuncSetSharedMemConfig, "cudaFuncSetSharedMemConfig");
      bindFunc(cast(void**)&cudaFuncGetAttributes, "cudaFuncGetAttributes");
      bindFunc(cast(void**)&cudaFuncSetAttribute, "cudaFuncSetAttribute");
      bindFunc(cast(void**)&cudaSetDoubleForDevice, "cudaSetDoubleForDevice");
      bindFunc(cast(void**)&cudaSetDoubleForHost, "cudaSetDoubleForHost");
      bindFunc(cast(void**)&cudaLaunchHostFunc, "cudaLaunchHostFunc");
      bindFunc(cast(void**)&cudaOccupancyMaxActiveBlocksPerMultiprocessor, "cudaOccupancyMaxActiveBlocksPerMultiprocessor");
      bindFunc(cast(void**)&cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags, "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags");
      bindFunc(cast(void**)&cudaConfigureCall, "cudaConfigureCall");
      bindFunc(cast(void**)&cudaSetupArgument, "cudaSetupArgument");
      bindFunc(cast(void**)&cudaLaunch, "cudaLaunch");
      bindFunc(cast(void**)&cudaMallocManaged, "cudaMallocManaged");
      bindFunc(cast(void**)&cudaMalloc, "cudaMalloc");
      bindFunc(cast(void**)&cudaMallocHost, "cudaMallocHost");
      bindFunc(cast(void**)&cudaMallocPitch, "cudaMallocPitch");
      bindFunc(cast(void**)&cudaMallocArray, "cudaMallocArray");
      bindFunc(cast(void**)&cudaFree, "cudaFree");
      bindFunc(cast(void**)&cudaFreeHost, "cudaFreeHost");
      bindFunc(cast(void**)&cudaFreeArray, "cudaFreeArray");
      bindFunc(cast(void**)&cudaFreeMipmappedArray, "cudaFreeMipmappedArray");
      bindFunc(cast(void**)&cudaHostAlloc, "cudaHostAlloc");
      bindFunc(cast(void**)&cudaHostRegister, "cudaHostRegister");
      bindFunc(cast(void**)&cudaHostUnregister, "cudaHostUnregister");
      bindFunc(cast(void**)&cudaHostGetDevicePointer, "cudaHostGetDevicePointer");
      bindFunc(cast(void**)&cudaHostGetFlags, "cudaHostGetFlags");
      bindFunc(cast(void**)&cudaMalloc3D, "cudaMalloc3D");
      bindFunc(cast(void**)&cudaMalloc3DArray, "cudaMalloc3DArray");
      bindFunc(cast(void**)&cudaMallocMipmappedArray, "cudaMallocMipmappedArray");
      bindFunc(cast(void**)&cudaGetMipmappedArrayLevel, "cudaGetMipmappedArrayLevel");
      bindFunc(cast(void**)&cudaMemcpy3D, "cudaMemcpy3D");
      bindFunc(cast(void**)&cudaMemcpy3DPeer, "cudaMemcpy3DPeer");
      bindFunc(cast(void**)&cudaMemcpy3DAsync, "cudaMemcpy3DAsync");
      bindFunc(cast(void**)&cudaMemcpy3DPeerAsync, "cudaMemcpy3DPeerAsync");
      bindFunc(cast(void**)&cudaMemGetInfo, "cudaMemGetInfo");
      bindFunc(cast(void**)&cudaArrayGetInfo, "cudaArrayGetInfo");
      bindFunc(cast(void**)&cudaMemcpy, "cudaMemcpy");
      bindFunc(cast(void**)&cudaMemcpyPeer, "cudaMemcpyPeer");
      bindFunc(cast(void**)&cudaMemcpyToArray, "cudaMemcpyToArray");
      bindFunc(cast(void**)&cudaMemcpyFromArray, "cudaMemcpyFromArray");
      bindFunc(cast(void**)&cudaMemcpyArrayToArray, "cudaMemcpyArrayToArray");
      bindFunc(cast(void**)&cudaMemcpy2D, "cudaMemcpy2D");
      bindFunc(cast(void**)&cudaMemcpy2DToArray, "cudaMemcpy2DToArray");
      bindFunc(cast(void**)&cudaMemcpy2DFromArray, "cudaMemcpy2DFromArray");
      bindFunc(cast(void**)&cudaMemcpy2DArrayToArray, "cudaMemcpy2DArrayToArray");
      bindFunc(cast(void**)&cudaMemcpyToSymbol, "cudaMemcpyToSymbol");
      bindFunc(cast(void**)&cudaMemcpyFromSymbol, "cudaMemcpyFromSymbol");
      bindFunc(cast(void**)&cudaMemcpyAsync, "cudaMemcpyAsync");
      bindFunc(cast(void**)&cudaMemcpyPeerAsync, "cudaMemcpyPeerAsync");
      bindFunc(cast(void**)&cudaMemcpyToArrayAsync, "cudaMemcpyToArrayAsync");
      bindFunc(cast(void**)&cudaMemcpyFromArrayAsync, "cudaMemcpyFromArrayAsync");
      bindFunc(cast(void**)&cudaMemcpy2DAsync, "cudaMemcpy2DAsync");
      bindFunc(cast(void**)&cudaMemcpy2DToArrayAsync, "cudaMemcpy2DToArrayAsync");
      bindFunc(cast(void**)&cudaMemcpy2DFromArrayAsync, "cudaMemcpy2DFromArrayAsync");
      bindFunc(cast(void**)&cudaMemcpyToSymbolAsync, "cudaMemcpyToSymbolAsync");
      bindFunc(cast(void**)&cudaMemcpyFromSymbolAsync, "cudaMemcpyFromSymbolAsync");
      bindFunc(cast(void**)&cudaMemset, "cudaMemset");
      bindFunc(cast(void**)&cudaMemset2D, "cudaMemset2D");
      bindFunc(cast(void**)&cudaMemset3D, "cudaMemset3D");
      bindFunc(cast(void**)&cudaMemsetAsync, "cudaMemsetAsync");
      bindFunc(cast(void**)&cudaMemset2DAsync, "cudaMemset2DAsync");
      bindFunc(cast(void**)&cudaMemset3DAsync, "cudaMemset3DAsync");
      bindFunc(cast(void**)&cudaGetSymbolAddress, "cudaGetSymbolAddress");
      bindFunc(cast(void**)&cudaGetSymbolSize, "cudaGetSymbolSize");
      bindFunc(cast(void**)&cudaMemPrefetchAsync, "cudaMemPrefetchAsync");
      bindFunc(cast(void**)&cudaMemAdvise, "cudaMemAdvise");
      bindFunc(cast(void**)&cudaMemRangeGetAttribute, "cudaMemRangeGetAttribute");
      bindFunc(cast(void**)&cudaMemRangeGetAttributes, "cudaMemRangeGetAttributes");
      bindFunc(cast(void**)&cudaPointerGetAttributes, "cudaPointerGetAttributes");
      bindFunc(cast(void**)&cudaDeviceCanAccessPeer, "cudaDeviceCanAccessPeer");
      bindFunc(cast(void**)&cudaDeviceEnablePeerAccess, "cudaDeviceEnablePeerAccess");
      bindFunc(cast(void**)&cudaDeviceDisablePeerAccess, "cudaDeviceDisablePeerAccess");
      bindFunc(cast(void**)&cudaGraphicsUnregisterResource, "cudaGraphicsUnregisterResource");
      bindFunc(cast(void**)&cudaGraphicsResourceSetMapFlags, "cudaGraphicsResourceSetMapFlags");
      bindFunc(cast(void**)&cudaGraphicsMapResources, "cudaGraphicsMapResources");
      bindFunc(cast(void**)&cudaGraphicsUnmapResources, "cudaGraphicsUnmapResources");
      bindFunc(cast(void**)&cudaGraphicsResourceGetMappedPointer, "cudaGraphicsResourceGetMappedPointer");
      bindFunc(cast(void**)&cudaGraphicsSubResourceGetMappedArray, "cudaGraphicsSubResourceGetMappedArray");
      bindFunc(cast(void**)&cudaGraphicsResourceGetMappedMipmappedArray, "cudaGraphicsResourceGetMappedMipmappedArray");
      bindFunc(cast(void**)&cudaGetChannelDesc, "cudaGetChannelDesc");
      bindFunc(cast(void**)&cudaBindTexture, "cudaBindTexture");
      bindFunc(cast(void**)&cudaBindTexture2D, "cudaBindTexture2D");
      bindFunc(cast(void**)&cudaBindTextureToArray, "cudaBindTextureToArray");
      bindFunc(cast(void**)&cudaBindTextureToMipmappedArray, "cudaBindTextureToMipmappedArray");
      bindFunc(cast(void**)&cudaUnbindTexture, "cudaUnbindTexture");
      bindFunc(cast(void**)&cudaGetTextureAlignmentOffset, "cudaGetTextureAlignmentOffset");
      bindFunc(cast(void**)&cudaGetTextureReference, "cudaGetTextureReference");
      bindFunc(cast(void**)&cudaBindSurfaceToArray, "cudaBindSurfaceToArray");
      bindFunc(cast(void**)&cudaGetSurfaceReference, "cudaGetSurfaceReference");
      bindFunc(cast(void**)&cudaCreateTextureObject, "cudaCreateTextureObject");
      bindFunc(cast(void**)&cudaDestroyTextureObject, "cudaDestroyTextureObject");
      bindFunc(cast(void**)&cudaGetTextureObjectResourceDesc, "cudaGetTextureObjectResourceDesc");
      bindFunc(cast(void**)&cudaGetTextureObjectTextureDesc, "cudaGetTextureObjectTextureDesc");
      bindFunc(cast(void**)&cudaGetTextureObjectResourceViewDesc, "cudaGetTextureObjectResourceViewDesc");
      bindFunc(cast(void**)&cudaCreateSurfaceObject, "cudaCreateSurfaceObject");
      bindFunc(cast(void**)&cudaDestroySurfaceObject, "cudaDestroySurfaceObject");
      bindFunc(cast(void**)&cudaGetSurfaceObjectResourceDesc, "cudaGetSurfaceObjectResourceDesc");
      bindFunc(cast(void**)&cudaDriverGetVersion, "cudaDriverGetVersion");
      bindFunc(cast(void**)&cudaRuntimeGetVersion, "cudaRuntimeGetVersion");
      bindFunc(cast(void**)&cudaGraphCreate, "cudaGraphCreate");
      bindFunc(cast(void**)&cudaGraphAddKernelNode, "cudaGraphAddKernelNode");
      bindFunc(cast(void**)&cudaGraphKernelNodeGetParams, "cudaGraphKernelNodeGetParams");
      bindFunc(cast(void**)&cudaGraphKernelNodeSetParams, "cudaGraphKernelNodeSetParams");
      bindFunc(cast(void**)&cudaGraphAddMemcpyNode, "cudaGraphAddMemcpyNode");
      bindFunc(cast(void**)&cudaGraphMemcpyNodeGetParams, "cudaGraphMemcpyNodeGetParams");
      bindFunc(cast(void**)&cudaGraphMemcpyNodeSetParams, "cudaGraphMemcpyNodeSetParams");
      bindFunc(cast(void**)&cudaGraphAddMemsetNode, "cudaGraphAddMemsetNode");
      bindFunc(cast(void**)&cudaGraphMemsetNodeGetParams, "cudaGraphMemsetNodeGetParams");
      bindFunc(cast(void**)&cudaGraphMemsetNodeSetParams, "cudaGraphMemsetNodeSetParams");
      bindFunc(cast(void**)&cudaGraphAddHostNode, "cudaGraphAddHostNode");
      bindFunc(cast(void**)&cudaGraphHostNodeGetParams, "cudaGraphHostNodeGetParams");
      bindFunc(cast(void**)&cudaGraphHostNodeSetParams, "cudaGraphHostNodeSetParams");
      bindFunc(cast(void**)&cudaGraphAddChildGraphNode, "cudaGraphAddChildGraphNode");
      bindFunc(cast(void**)&cudaGraphChildGraphNodeGetGraph, "cudaGraphChildGraphNodeGetGraph");
      bindFunc(cast(void**)&cudaGraphAddEmptyNode, "cudaGraphAddEmptyNode");
      bindFunc(cast(void**)&cudaGraphClone, "cudaGraphClone");
      bindFunc(cast(void**)&cudaGraphNodeFindInClone, "cudaGraphNodeFindInClone");
      bindFunc(cast(void**)&cudaGraphNodeGetType, "cudaGraphNodeGetType");
      bindFunc(cast(void**)&cudaGraphGetNodes, "cudaGraphGetNodes");
      bindFunc(cast(void**)&cudaGraphGetRootNodes, "cudaGraphGetRootNodes");
      bindFunc(cast(void**)&cudaGraphGetEdges, "cudaGraphGetEdges");
      bindFunc(cast(void**)&cudaGraphNodeGetDependencies, "cudaGraphNodeGetDependencies");
      bindFunc(cast(void**)&cudaGraphNodeGetDependentNodes, "cudaGraphNodeGetDependentNodes");
      bindFunc(cast(void**)&cudaGraphAddDependencies, "cudaGraphAddDependencies");
      bindFunc(cast(void**)&cudaGraphRemoveDependencies, "cudaGraphRemoveDependencies");
      bindFunc(cast(void**)&cudaGraphDestroyNode, "cudaGraphDestroyNode");
      bindFunc(cast(void**)&cudaGraphInstantiate, "cudaGraphInstantiate");
      bindFunc(cast(void**)&cudaGraphLaunch, "cudaGraphLaunch");
      bindFunc(cast(void**)&cudaGraphExecDestroy, "cudaGraphExecDestroy");
      bindFunc(cast(void**)&cudaGraphDestroy, "cudaGraphDestroy");
      bindFunc(cast(void**)&cudaGetExportTable, "cudaGetExportTable");
    }
  }

  public
  {
    this()
    {
      super(libNames);
    }
  }
}

__gshared DerelictCUDARuntimeLoader DerelictCUDARuntime;

shared static this()
{
    DerelictCUDARuntime = new DerelictCUDARuntimeLoader();
}
